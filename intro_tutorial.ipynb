{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8yw_hrKYuL6"
   },
   "source": [
    "# Introduction to Colab, JAX, haiku\n",
    "\n",
    "Authors: David Szepesvari, Viorica Patraucean \n",
    "\n",
    "Contact: vpatrauc@gmail.com\n",
    "\n",
    "Thanks to Carl Doersch and Stanislaw Jastrzebski for proofreading and advice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsr0IKnOYEUi"
   },
   "source": [
    "## What is Colab?\n",
    "\n",
    "[Colaboratory](https://colab.sandbox.google.com/notebooks/welcome.ipynb) is a [Jupyter](http://jupyter.org/) notebook environment that requires no setup to use. It allows you to create and share documents that contain\n",
    "\n",
    "* Live, runnable code\n",
    "* Visualizations\n",
    "* Explanatory text\n",
    "\n",
    "It's also a great tool for prototyping and quick development. Let's give it a try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZjpiZSDYJ1R"
   },
   "source": [
    "Run the following so-called *(Code) Cell* by moving the cursor into it, and either\n",
    "\n",
    "* Pressing the \"play\" icon on the left of the cell, or\n",
    "* Hitting **`Shift + Enter`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VIRY1OxWYNED"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, virtualEEML2021!\n"
     ]
    }
   ],
   "source": [
    "print('Hello, virtualEEML2021!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okXieYk8YSZ5"
   },
   "source": [
    "You should see the `Hello, virtualEEML2021!` printed under the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFIWmbiyYb_A"
   },
   "source": [
    "The code is executed on a virtual machine dedicated to your account, with the results sent back to your browser. This has some positive and negative consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXzTe0-8Yk12"
   },
   "source": [
    "### Using a GPU\n",
    "\n",
    "You can connect to a virtual machine with a GPU. To select the hardware you want to use, follow either\n",
    "\n",
    "* **Edit > Notebook settings**, or\n",
    "* **Runtime > Change runtime type**\n",
    "\n",
    "and choose an accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USfPlwlyY0oK"
   },
   "source": [
    "### Losing Connection\n",
    "\n",
    "You may lose connection to your virtual machine. The two most common causes are\n",
    "\n",
    "* Virtual machines are recycled when idle for a while, and have a maximum lifetime enforced by the system.\n",
    "* Long-running background computations, particularly on GPUs, may be stopped.\n",
    "\n",
    "**If you lose connection**, the state of your notebook will also be lost. You will need to **rerun all cells** up to the one you are currently working on. To do so\n",
    "\n",
    "1. Select (place the cursor into) the cell you are working on. \n",
    "2. Follow **Runtime > Run before**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWgJEOspaQA_"
   },
   "source": [
    "### Pretty Printing by colab\n",
    "1) If the **last operation** of a given cell returns a value, it will be pretty printed by colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ttk-hcG2aT7M"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C8hMw-vRaXP1"
   },
   "outputs": [],
   "source": [
    "my_dict = {'one': 1, 'some set': {4, 2, 2}, 'a regular list': range(5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNs8mVyYaa-n"
   },
   "source": [
    "There is no output from the second cell, as assignment does not return anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg4BV2bXaeqT"
   },
   "source": [
    "2) You can explicitly **print** anything before the last operation, or **supress** the output of the last operation by adding a semicolon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mn0aHa_pajOT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': 1, 'some set': {2, 4}, 'a regular list': range(0, 5)}\n"
     ]
    }
   ],
   "source": [
    "print(my_dict)\n",
    "my_dict['one'] * 10 + 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFVpqvB-ZTot"
   },
   "source": [
    "### Scoping and Execution Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdeQn9ExapJM"
   },
   "source": [
    "Notice that in the previous code cell we worked with `my_dict`, while it was defined in an even earlier cell.\n",
    "\n",
    "1) In colabs, variables defined at cell root have **global** scope.\n",
    "\n",
    "Modify `my_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wx4_0k3Ka6KR"
   },
   "outputs": [],
   "source": [
    "my_dict['I\\'ve been changed!'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsXP0Spya_NG"
   },
   "source": [
    "2) Cells can be **run** in any **arbitrary order**, and global state is maintained between them.\n",
    "\n",
    "Try re-running the cell where we printed `my_dict`. You should see now  see the additional item `\"I've been changed!\": True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFofwmqAZP6s"
   },
   "source": [
    "3) Unintentionally reusing a global variable can lead to bugs. If all else fails, you can uncomment and run the following line to **clear all global variables** and run again all the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ugbgF-kCZoE-"
   },
   "outputs": [],
   "source": [
    "# %reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8n7dyqqZ5sf"
   },
   "source": [
    "### Autocomplete / Documentation\n",
    "\n",
    "* Pressing *`<TAB>`* after typing a prefix will show the available variables / commands.\n",
    "* Pressing *`<TAB>`* on a function parameter list will show the function documentation.\n",
    "\n",
    "Note: this only works for variables that have already been defined (not while you are writing your code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRWQOmFKbSNW"
   },
   "source": [
    "### Setup and Imports\n",
    "\n",
    "Python packages can and need to be imported into your colab notebook, the same way you would import them in a python script. For example, to use `numpy`, you would do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oLiqqd44bg3B"
   },
   "outputs": [],
   "source": [
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9_Kj04gbXsf"
   },
   "source": [
    "While many packages can just be imported, some (e.g. `haiku`, a neural network library from DeepMind) may not be prepackaged in the runtime. With Colab, you can install any python package from `pip` for the duration of your connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SAmMS5h8bn_1"
   },
   "outputs": [],
   "source": [
    "# we will use haiku on top of jax \n",
    "# !pip install -q dm-haiku\n",
    "# import haiku as hk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXYkeehcb8kg"
   },
   "source": [
    "### Forms\n",
    "\n",
    "With colab it is easy to take input from the user in code cells through so called forms. A simplest example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IAZhaY6NcXlc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a+b = 5\n"
     ]
    }
   ],
   "source": [
    "#@title This text shows up as a title.\n",
    "\n",
    "a = 2  #@param {type: 'integer'}\n",
    "b = 3  #@param\n",
    "\n",
    "print('a+b =', str(a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDlY63E-ccL7"
   },
   "source": [
    "You can change parameters on the right hand side, then rerun the cell to use these values. **Try setting the value of a=5 and rerun the cell above.**\n",
    "\n",
    "In order to expose a variable as parameter you just add `#@param` after it. There are various kinds of params, if you're interested you can read more about this on the official starting colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_RVl7E8cgut"
   },
   "source": [
    "Cells with forms allow you to toggle whether\n",
    "\n",
    "* the code,\n",
    "* the form,\n",
    "* or both\n",
    "\n",
    "are visible.\n",
    "\n",
    "**Try switching between these 3 options for the above cell.** This is how you do this:\n",
    "\n",
    "1. Click anywhere over the area of the cell with the form to highlight it.\n",
    "2. Click on the \"three vertically arranged dots\" icon in the top right of the cell.\n",
    "3. Go to \"Form >\", select your desired action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjPF8rKiize7"
   },
   "source": [
    "## JAX\n",
    "[JAX](https://jax.readthedocs.io/en/latest/jax.html) allows NumPy-like code to execute on CPU, or accelerators like GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n",
    "\n",
    "- JAX automatically differentiates python code and NumPy code (with [Autograd](https://github.com/hips/autograd))\n",
    "- uses [XLA](https://www.tensorflow.org/xla) to compile and run NumPy code efficiently on accelerators\n",
    "\n",
    "This makes JAX a great tool for high-performance numerical computing and machine learning research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE4j9mLOQFpF"
   },
   "source": [
    "**Key Concepts:**\n",
    "\n",
    "* JAX provides a NumPy-inspired interface for convenience.\n",
    "* Through duck-typing, JAX arrays can often be used as drop-in replacements of NumPy arrays.\n",
    "* Unlike NumPy arrays, JAX arrays are always immutable.\n",
    "\n",
    "JAX has a functional interface, that is, all functions are pure.\n",
    "\n",
    "Various neural network libraries have been built on top of JAX to enable fast research and provide more familiar object oriented interfaces. We will see two of these below: haiku and flax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDmbQoL5mYmA"
   },
   "source": [
    "### JAX and random number generators\n",
    "Unlike many ML frameworks, JAX does not hide the pseudo-random number generator state. You need to generate explicitely a random key, and pass it to the operations that work with random numbers (e.g. initialising a model, dropout etc). A call to a random function with the same key does not change the state of the generator. This has to be done explicitely with `split()` or `next_rng_key()` in `haiku`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0j5kLgolmlCl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.8160858  -0.48262328  0.339889  ]\n",
      "[ 1.8160858  -0.48262328  0.339889  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "key = random.PRNGKey(0)\n",
    "x1 = random.normal(key, (3,))\n",
    "print(x1)\n",
    "x2 = random.normal(key, (3,))\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IZNs5VAdfmpG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1389316   0.5093349  -0.53116125]\n",
      "[ 1.1378784  -1.220955   -0.59153646]\n"
     ]
    }
   ],
   "source": [
    "# Let's split the key to be able to generate different random values\n",
    "key, new_key = random.split(key)\n",
    "x1 = random.normal(key, (3,))\n",
    "print (x1)\n",
    "x2 = random.normal(new_key, (3,))\n",
    "print (x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nwjljk9RhDh"
   },
   "source": [
    "Each time you need to use randomness, split a key and use one for your needs, the other to split later on. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "j-KVDJDNSCbO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8076067  -0.24184921]\n",
      "[0.57522273 0.00425153]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(1)\n",
    "# Do things..\n",
    "#\n",
    "# Need a sample from a normal distribution:\n",
    "key, subkey = random.split(key)\n",
    "print(random.normal(subkey, (2,)))\n",
    "# Do other things..\n",
    "#\n",
    "# Need another sample from a normal distribution:\n",
    "key, subkey = random.split(key)\n",
    "print(random.normal(subkey, (2,)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hiqCH1Nkvdv"
   },
   "source": [
    "### JAX program transformations with examples \n",
    "* `jit` (just-in-time compilation) -- speeds up your code by running all the ops inside the jit-ed function as a *fused* op; it compiles the function when it's called the first time and uses the compiled (optimised) version from the second call onwards.\n",
    "* `grad` -- returns derivatives of function with respect to the model weights passed as parameters\n",
    "* `vmap` -- automatic batching; returns a new function that can apply the original (per-sample) function to a batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qiWR4CPjlc6T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.51 ms ± 202 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "243 µs ± 11.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import grad, jit\n",
    "# Let's use jit to speed up a function\n",
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "# execute the function without jit\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()   # block_until_ready is needed as jax, by default, runs operations asyncronously\n",
    "\n",
    "# Execute the function with jit and compare timing with above -- it should be much faster\n",
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rP70-aR3oouX"
   },
   "outputs": [],
   "source": [
    "# Let's use grad to compute gradient of a simple function\n",
    "def simple_fun(x):\n",
    "  return jnp.sin(x) / x\n",
    "\n",
    "# Get the gradient of simple_fun with respect to x\n",
    "grad_simple_fun = grad(simple_fun)\n",
    "\n",
    "# We can also get higher order derivatives, e.g. Hessian\n",
    "grad_grad_simple_fun = grad(grad(simple_fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FH8uTx364T0m"
   },
   "outputs": [],
   "source": [
    "# Let's plot the result\n",
    "import matplotlib.pyplot as plt\n",
    "x_range = jnp.arange(-8, 8, .1)\n",
    "plt.plot(x_range, simple_fun(x_range), 'b')\n",
    "plt.plot(x_range, [grad_simple_fun(xi) for xi in x_range], 'r')\n",
    "plt.plot(x_range, [grad_grad_simple_fun(xi) for xi in x_range], '--g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foOt9Hq2_dqI"
   },
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "# Let's see how vmap can be used to vectorize computations efficiently\n",
    "# In the example above, we can use vmap instead of loop to compute gradients\n",
    "\n",
    "grad_vect_simple_fun = vmap(grad_simple_fun)(x_range)\n",
    "\n",
    "# plot again and check that the gradients are identical \n",
    "plt.plot(x_range, simple_fun(x_range), 'b')\n",
    "plt.plot(x_range, [grad_simple_fun(xi) for xi in x_range], 'r')\n",
    "plt.plot(x_range, grad_vect_simple_fun, 'oc', mfc='none')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dibhhYsjphE3"
   },
   "outputs": [],
   "source": [
    "# Let's time them!\n",
    "\n",
    "# naive batching\n",
    "def naively_batched(x):\n",
    "  return jnp.stack([grad_simple_fun(xi) for xi in x])\n",
    "\n",
    "# manual batching with jit\n",
    "@jit\n",
    "def manual_batched(x):\n",
    "  return jnp.stack([grad_simple_fun(xi) for xi in x])\n",
    "\n",
    "# Batching using vmap and jit\n",
    "@jit\n",
    "def vmap_batched(x):\n",
    "  return vmap(grad_simple_fun)(x)\n",
    "\n",
    "print ('Naively batched')\n",
    "%timeit naively_batched(x_range).block_until_ready()\n",
    "print ('jit batched')\n",
    "%timeit manual_batched(x_range).block_until_ready()\n",
    "print ('With jit vmap')\n",
    "%timeit vmap_batched(x_range).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EdK9zpqUl2H"
   },
   "source": [
    "## Pytrees\n",
    "\n",
    "The jax ecosystem (including flax and haiku) relies on structured, nested data -- [pytrees](https://jax.readthedocs.io/en/latest/pytrees.html).\n",
    "\n",
    "*In JAX, a pytree is a container of leaf elements and/or more pytrees. Containers include lists, tuples, and dicts (JAX can be extended to consider other container types as pytrees [...]). A leaf element is anything that’s not a pytree, e.g. an array. In other words, a pytree is just a possibly-nested standard or user-registered Python container. If nested, note that the container types do not need to match. A single “leaf”, i.e. a non-container object, is also considered a pytree.*\n",
    "\n",
    "Basically a pytree is a generic nested python container, that gives you more control over what structures should act like leaves vs branching containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwFnfj4_JiM6"
   },
   "source": [
    "Let's see an example (taken from [this tutorial](https://colab.sandbox.google.com/github/google/jax/blob/master/docs/jax-101/05.1-pytrees.ipynb#scrollTo=9UjxVY9ulSCn)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjhUrw-NUzfz"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "example_trees = [\n",
    "    [1, 'a', object()],\n",
    "    (1, (2, 3), ()),\n",
    "    [1, {'k1': 2, 'k2': (3, 4)}, 5],\n",
    "    {'a': 2, 'b': (2, 3)},\n",
    "    jnp.array([1, 2, 3]),\n",
    "]\n",
    "\n",
    "# Let's see how many leaves they have, by using `jax.tree_leaves(pytree)` \n",
    "# to access the flattened leaves of the tree\n",
    "for pytree in example_trees:\n",
    "  leaves = jax.tree_leaves(pytree)\n",
    "  print(f\"{repr(pytree):<45} has {len(leaves)} leaves: {leaves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LInfkv5-Kxr0"
   },
   "source": [
    "Places where you commonly find pytrees are:\n",
    "* Model parameters (e.g. see `get_num_params` function below)\n",
    "* Dataset entries\n",
    "* RL agent observations\n",
    "\n",
    "Check the [tutorial linked above](https://colab.sandbox.google.com/github/google/jax/blob/master/docs/jax-101/05.1-pytrees.ipynb#scrollTo=-h05_PNNhZ-D) for more details and exercises on pytrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSMUMtokL4rO"
   },
   "source": [
    "### For more details on jax, check out this [collection of eight JAX-101 tutorials](https://jax.readthedocs.io/en/latest/jax-101/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0lHGcAwdfQq"
   },
   "source": [
    "### Also, read the doc for [common gotchas](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) in JAX!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7ViyN6Q8kOa"
   },
   "source": [
    "## Haiku -- object-oriented neural network library on top of JAX\n",
    "\n",
    "[Haiku](https://github.com/deepmind/dm-haiku) is a simple neural network library for JAX that enables users to use familiar object-oriented programming models while allowing full access to JAX's pure function transformations.\n",
    "\n",
    "This colab goes through a complete but minimal example training an MLP classifier on MNIST. The [quickstart](https://github.com/deepmind/dm-haiku#quickstart) and [user-manual](https://github.com/deepmind/dm-haiku#user-manual) provide more information.\n",
    "\n",
    "Notable functions / entities\n",
    "* `hk.Module` base class: implement your own modules by deriving from it\n",
    "* `hk.transform`: convert non-pure (objects) functions into pure functions; returns an object with a pair of pure functions `init` and `apply`.\n",
    "* `hk.next_rng_key()`: returns a unique random key\n",
    "\n",
    "**Important.**\n",
    "Do not use jax transforms (grad, jit, etc) with impure functions (e.g. inside Haiku networks). Instead, use them on `hk.transform`ed pure functions, or try the experimental `haiku.grad` and `haiku.jit` etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h4MVQzkck_R"
   },
   "source": [
    "### Example: Train MLP classifier on MNIST\n",
    "\n",
    "We also use\n",
    "\n",
    "* [optax](https://github.com/deepmind/optax) a gradient processing and optimization library.\n",
    "* tensorflow datasets to load and pre-process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NG3krUq1WK_A"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Mapping, Generator, Tuple \n",
    "\n",
    "# we will use haiku on top of jax \n",
    "!pip install -q dm-haiku optax\n",
    "import haiku as hk\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import enum\n",
    "\n",
    "# Dataset library\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Plotting library.\n",
    "from matplotlib import pyplot as plt\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n",
    "devices = jax.devices()\n",
    "if not str(devices[0]).startswith('gpu'):\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(devices[0]))\n",
    "\n",
    "# define some useful types\n",
    "OptState = Any\n",
    "Batch = Mapping[str, np.ndarray]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6r0EfeAqIZkS"
   },
   "source": [
    "### Define the dataset: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDP4M0aVWpdB"
   },
   "outputs": [],
   "source": [
    "# We use TF datasets; JAX does not support data loading or preprocessing.\n",
    "NUM_CLASSES = 10  # MNIST has 10 classes, corresponding to the different digits.\n",
    "def load_dataset(\n",
    "    split: str,\n",
    "    *,\n",
    "    is_training: bool,\n",
    "    batch_size: int,\n",
    ") -> Generator[Batch, None, None]:\n",
    "  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
    "  ds = tfds.load('mnist:3.*.*', split=split).cache().repeat()\n",
    "  if is_training:\n",
    "    ds = ds.shuffle(10 * batch_size, seed=0)\n",
    "  ds = ds.batch(batch_size)\n",
    "  return tfds.as_numpy(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2OYWEKKgVEJ"
   },
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "MAX_IMAGES = 10\n",
    "def gallery(images, label, title='Input images'):  \n",
    "  class_dict = [u'zero', u'one', u'two', u'three', u'four', u'five', u'six', u'seven', u'eight', u'nine']\n",
    "  num_frames, h, w, num_channels = images.shape\n",
    "  num_frames = min(num_frames, MAX_IMAGES)\n",
    "  ff, axes = plt.subplots(1, num_frames,\n",
    "                          figsize=(30, 30),\n",
    "                          subplot_kw={'xticks': [], 'yticks': []})\n",
    "  if images.min() < 0:\n",
    "    images = (images + 1.) / 2.\n",
    "  for i in range(0, num_frames):\n",
    "    if num_channels == 3:\n",
    "      axes[i].imshow(np.squeeze(images[i]))\n",
    "    else:\n",
    "      axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n",
    "    axes[i].set_title(class_dict[label[i]], fontsize=28)\n",
    "    plt.setp(axes[i].get_xticklabels(), visible=False)\n",
    "    plt.setp(axes[i].get_yticklabels(), visible=False)\n",
    "  ff.subplots_adjust(wspace=0.1)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTtne8cbZABG"
   },
   "outputs": [],
   "source": [
    "# Display some training images with their labels.\n",
    "# First, create a dataset iterator for fetching batches.\n",
    "display_dataset_iter = iter(load_dataset('train', is_training=True, batch_size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2VFyzjwZXbQ"
   },
   "outputs": [],
   "source": [
    "# Then get a batch and display it.\n",
    "display_batch = next(display_dataset_iter)\n",
    "gallery(display_batch['image'], display_batch['label'])\n",
    "\n",
    "# To iterate over the dataset, one could also do\n",
    "# for batch in load_dataset('train', is_training=True, batch_size=10):\n",
    "#   ..process batch.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mheFhe-gbe9W"
   },
   "outputs": [],
   "source": [
    "# And check their shapes.\n",
    "print(display_batch['image'].shape)\n",
    "print(display_batch['label'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyL8nAD-OTn9"
   },
   "outputs": [],
   "source": [
    "#@title Training hyperparams\n",
    "# There are 60k examples in the training\n",
    "# data. We consider training batches of 1000 examples.\n",
    "# We train for 10 epochs with constant learning rate\n",
    "# of 1e-3.\n",
    "# We use weight decay (L2 regularization), adding\n",
    "# the sum of the l2 norms of the weights to the\n",
    "# loss with a weight of 1e-4.\n",
    "TRAIN_BATCH_SIZE = 1000  #@param\n",
    "NUM_EPOCHS = 10  #@param\n",
    "lr = 1e-3 #@param\n",
    "WEIGHT_DECAY = 1e-4  #@param\n",
    "\n",
    "TRAIN_NUM_EXAMPLES = 60000  # Number of training examples in MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMXfrKnof2L3"
   },
   "outputs": [],
   "source": [
    "# Make datasets for train and test\n",
    "train_dataset = load_dataset('train', is_training=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "train_eval_dataset = load_dataset('train', is_training=False, batch_size=10000)\n",
    "test_eval_dataset = load_dataset('test', is_training=False, batch_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00m3RHu0BTir"
   },
   "source": [
    "### Define classifier: a simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j58v6LhWjrQ"
   },
   "outputs": [],
   "source": [
    "def net_fn(batch: Batch) -> jnp.ndarray:\n",
    "  \"\"\"Standard LeNet-300-100 MLP network.\"\"\"\n",
    "  # The images are in [0, 255], uint8; we need to convert to float and normalise \n",
    "  x = batch['image'].astype(jnp.float32) / 255.\n",
    "  # We use hk.Sequential to chain the modules in the network\n",
    "  mlp = hk.Sequential([\n",
    "  # The input images are 28x28, so we first flatten them to apply linear (fully-connected) layers                     \n",
    "      hk.Flatten(),  \n",
    "      hk.Linear(300), jax.nn.relu,\n",
    "      hk.Linear(100), jax.nn.relu,\n",
    "      hk.Linear(NUM_CLASSES),\n",
    "  ])\n",
    "  return mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CH5-IvPkMftZ"
   },
   "source": [
    "### Retrieve pure functions for our model (`init`, `apply`) using `hk.transform`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9YzGJ_4WuE5"
   },
   "outputs": [],
   "source": [
    "# Since we don't store additional state statistics, e.g. needed in batch norm,\n",
    "# we use `hk.transform`. When we use batch_norm, we will use `hk.transform_with_state`.\n",
    "# Additionally, since we don't need to inject rng for this simple model, we also \n",
    "# apply `hk.without_apply_rng` to remove `rng` from `apply` function.\n",
    "net = hk.without_apply_rng(hk.transform(net_fn))\n",
    "# We will use the pure functions as net.init(...) and net.apply(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfm8NSdQOR1n"
   },
   "source": [
    "### Define the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxXMm5OzOPCO"
   },
   "outputs": [],
   "source": [
    "# We use Adam optimizer here. Others are possible, e.g. sgd with momentum.\n",
    "opt = optax.adam(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GA81esw-OoBK"
   },
   "source": [
    "### Define the optimisation objective (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OA1FdFSwWwtb"
   },
   "outputs": [],
   "source": [
    "# Training loss: cross-entropy plus weight decay regularization\n",
    "def loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "  \"\"\"Compute the loss of the network, including L2 for regularization.\"\"\"\n",
    "  \n",
    "  # Get network predictions\n",
    "  logits = net.apply(params, batch)\n",
    "\n",
    "  # Generate one_hot labels from index classes\n",
    "  labels = jax.nn.one_hot(batch['label'], NUM_CLASSES)\n",
    "\n",
    "  # Compute mean softmax cross entropy over the batch\n",
    "  softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
    "  softmax_xent /= labels.shape[0]\n",
    "\n",
    "  # Compute the weight decay loss by penalising the norm of parameters\n",
    "  l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_leaves(params))\n",
    "  \n",
    "  return softmax_xent + WEIGHT_DECAY * l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA_CZhFHQDDN"
   },
   "source": [
    "### Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlEMA3RCXBU_"
   },
   "outputs": [],
   "source": [
    "# Classification accuracy\n",
    "@jax.jit\n",
    "def accuracy(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "  # Get network predictions\n",
    "  predictions = net.apply(params, batch)\n",
    "  # Return accuracy = how many predictions match the ground truth\n",
    "  return jnp.mean(jnp.argmax(predictions, axis=-1) == batch['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SGCtelDQfOv"
   },
   "source": [
    "### Define training step (parameters update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsjUbkMWQlt-"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(\n",
    "    params: hk.Params,\n",
    "    opt_state: OptState,\n",
    "    batch: Batch,\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "  \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
    "  # Use jax transformation `grad` to compute gradients; \n",
    "  # it expects the prameters of the model and the input batch\n",
    "  grads = jax.grad(loss)(params, batch)\n",
    "\n",
    "  # Compute parameters updates based on gradients and optimiser state\n",
    "  updates, opt_state = opt.update(grads, opt_state)\n",
    "\n",
    "  # Apply updates to parameters\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "  return new_params, opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RubrTm2_WOiP"
   },
   "source": [
    "### Initialise the model, the optimiser and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a8lZDaAmH3B"
   },
   "outputs": [],
   "source": [
    "# Initialize the datasets model, optimiser. Note that a sample input is\n",
    "# needed to compute shapes of parameters.\n",
    "\n",
    "# Set up dataset iterators.\n",
    "train_ds_iterator = iter(train_dataset)\n",
    "train_eval_ds_iterator = iter(train_eval_dataset)\n",
    "test_eval_ds_iterator = iter(test_eval_dataset)\n",
    "\n",
    "# Draw a data batch\n",
    "batch = next(train_ds_iterator)\n",
    "# Initialize model\n",
    "params = net.init(jax.random.PRNGKey(42), batch)\n",
    "#Initialize optimiser\n",
    "opt_state = opt.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2yunmBjWyPQ"
   },
   "source": [
    "### Visualise data and parameter shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSmLtlnUgBeS"
   },
   "outputs": [],
   "source": [
    "# Display shapes and images\n",
    "print(batch['image'].shape)\n",
    "print(batch['label'].shape)\n",
    "gallery(batch['image'], batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euhMKRF_W5_D"
   },
   "outputs": [],
   "source": [
    "# Let's see how many parameters in our network and their shapes\n",
    "def get_num_params(params: hk.Params):\n",
    "  num_params = 0\n",
    "  for p in jax.tree_leaves(params): \n",
    "    print('param shape:', p.shape)\n",
    "    num_params = num_params + jnp.prod(jnp.array(p.shape))\n",
    "  return num_params\n",
    "print('Total number of parameters %d' % get_num_params(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq1OddpfX-6y"
   },
   "source": [
    "### Accuracy of the untrained model (should be ~10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHzblZx6X9jH"
   },
   "outputs": [],
   "source": [
    "# Run accuracy on the test dataset\n",
    "test_accuracy = accuracy(params, next(iter(test_eval_dataset)))\n",
    "print('Test accuracy %f '% test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIiw1dPDZEuN"
   },
   "outputs": [],
   "source": [
    "# Let's visualise some network predictions before training; if some are correct,\n",
    "# they are correct by chance.\n",
    "predictions = net.apply(params, batch)\n",
    "pred_labels = jnp.argmax(predictions, axis=-1)\n",
    "gallery(batch['image'], pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjeMJxKYaeN-"
   },
   "source": [
    "### Run one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejfhiuoiaiZ6"
   },
   "outputs": [],
   "source": [
    "# First, let's do one step and check if the updates lead to decrease in error\n",
    "loss_before_train = loss(params, batch) \n",
    "print('Loss before train %f' % loss_before_train)\n",
    "params, opt_state = update(params, opt_state, batch)\n",
    "new_loss = loss(params, next(train_ds_iterator))\n",
    "new_loss_same_batch = loss(params, batch)\n",
    "print('Loss after one step of training, same batch %f, different batch %f' % (new_loss_same_batch, new_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9QFlfNTW-GZ"
   },
   "source": [
    "### Run training steps in a loop. We also run evaluation periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAR5joBwV5cT"
   },
   "outputs": [],
   "source": [
    "# Train/eval loop.\n",
    "print(\"Training..\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  for step in range(TRAIN_NUM_EXAMPLES // TRAIN_BATCH_SIZE):\n",
    "    # Evaluate classification accuracy on train & test sets.\n",
    "    train_accuracy = accuracy(params, next(train_eval_ds_iterator))\n",
    "    test_accuracy = accuracy(params, next(test_eval_ds_iterator))\n",
    "    train_accuracy, test_accuracy = jax.device_get(\n",
    "        (train_accuracy, test_accuracy))\n",
    "\n",
    "    # Do SGD (Adam) on a batch of training examples.\n",
    "    params, opt_state = update(params, opt_state, next(train_ds_iterator))\n",
    "  print('Epoch %d Train / Test accuracy: %f / %f' % (\n",
    "      epoch+1, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNYLWtvmao4i"
   },
   "source": [
    "### Visualise network predictions after training; most of the predictions should be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGb8B6n6au9L"
   },
   "outputs": [],
   "source": [
    "# Get predictions for the same batch\n",
    "predictions = net.apply(params, batch)\n",
    "pred_labels = jnp.argmax(predictions, axis=-1)\n",
    "gallery(batch['image'], pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifFR1Iq9YChf"
   },
   "source": [
    "## Flax -- alternative library on top of JAX\n",
    "\n",
    "[Flax](https://github.com/google/flax) is another neural network library and ecosystem for JAX designed for flexibility.\n",
    "\n",
    "The link above provides a good quick intro, and the [documentation](https://flax.readthedocs.io/en/latest/index.html) has good examples, including an [annotated MNIST Example](https://colab.sandbox.google.com/github/google/flax/blob/master/docs/notebooks/annotated_mnist.ipynb). We reproduce a version of the MNIST example here, that is analogous to the Haiku section above.\n",
    "\n",
    "Flax comes with:\n",
    "\n",
    "* **Neural network API** (`flax.linen`): Dense, Conv, {Batch|Layer|Group} Norm, Attention, Pooling, {LSTM|GRU} Cell, Dropout\n",
    "* **Optimizers** (`flax.optim`): SGD, Momentum, Adam, LARS, Adagrad, LAMB, RMSprop\n",
    "* And much more, including utilities, worked examples and tuned, large scale examples.\n",
    "\n",
    "While flax includes optimizers, we can use `optax` to optimize the parameters in `flax.linen` networks. That is the approach we will take here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "narnOeEHkEqh"
   },
   "source": [
    "`haiku` and `flax` both provide an OOP interface for neural nets; they both produce `init()` and `apply()` functions for their modules. Some differences you will notice are:\n",
    "\n",
    "1. Slightly different ways of declaring and working with modules.\n",
    "2. Different signature for `apply()`.\n",
    "3. The common `flax` pattern of passing around a full \"TrainState\", while `haiku` tends to keep `params`, `optimizer_state`, etc separate.\n",
    "\n",
    "Due to these differences we will rewrite much of the training and evaluation functions from before -- even though the changes are fairly minimal.\n",
    "\n",
    "To showcase additional features, we will use a simple convnet and SGD with momentum optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kDzToFNj9SB"
   },
   "outputs": [],
   "source": [
    "!pip install -q flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQwghhw5j-kB"
   },
   "outputs": [],
   "source": [
    "from flax import linen as nn           # The Linen API\n",
    "from flax.training import train_state  # Useful dataclass to keep train state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PlPTAi9kqAQ"
   },
   "source": [
    "### Define network\n",
    "\n",
    "Create a convolutional neural network with the Linen API by subclassing\n",
    "[`Module`](https://flax.readthedocs.io/en/latest/flax.linen.html#core-module-abstraction).\n",
    "Because the architecture in this example is relatively simple—you're just\n",
    "stacking layers—you can define the inlined submodules directly within the\n",
    "`__call__` method and wrap it with the\n",
    "[`@compact`](https://flax.readthedocs.io/en/latest/flax.linen.html#compact-methods)\n",
    "decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bvZuQi6kg8T"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = x.astype(jnp.float32) / 255\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=NUM_CLASSES)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJJqE7gKpmye"
   },
   "source": [
    "We do not need to explicitly retrieve pure functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNABQ8yolWFK"
   },
   "source": [
    "### Create train state (and optimizer)\n",
    "\n",
    "A common pattern in Flax is to create a single dataclass that represents the\n",
    "entire training state, including step number, parameters, and optimizer state.\n",
    "\n",
    "Also adding optimizer & model to this state has the advantage that we only need\n",
    "to pass around a single argument to functions like `train_step()` (see below).\n",
    "\n",
    "Because this is such a common pattern, Flax provides the class\n",
    "[flax.training.train_state.TrainState](https://flax.readthedocs.io/en/latest/flax.training.html#train-state)\n",
    "that serves most basic use cases. Usually one would subclass it to add more data\n",
    "to be tracked, but in this example we can use it without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqpHZf4jlqvz"
   },
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate, momentum):\n",
    "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "  cnn = CNN()\n",
    "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "  # In the haiku example, we used Adam. Let's use SGD with momentum here. \n",
    "  tx = optax.sgd(learning_rate, momentum)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=cnn.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_62B7uilq2V"
   },
   "source": [
    "### Define the optimisation objective (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUgmndZ2p8Qk"
   },
   "outputs": [],
   "source": [
    "def flax_loss_fn(params, batch):\n",
    "  logits = CNN().apply({'params': params}, batch['image'])\n",
    "\n",
    "  labels = jax.nn.one_hot(batch['label'], NUM_CLASSES)\n",
    "\n",
    "  # Compute mean softmax cross entropy over the batch\n",
    "  softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
    "  softmax_xent /= labels.shape[0]\n",
    "\n",
    "  # Compute the weight decay loss by penalising the norm of parameters\n",
    "  l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in jax.tree_leaves(params))\n",
    "\n",
    "  return softmax_xent + WEIGHT_DECAY * l2_loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQ1z4jIyp8Wl"
   },
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajBJc-syra8p"
   },
   "outputs": [],
   "source": [
    "# Classification accuracy\n",
    "@jax.jit\n",
    "def flax_accuracy(params, batch):\n",
    "  # Get network predictions\n",
    "  predictions = CNN().apply({'params': params}, batch['image'])\n",
    "  # Return accuracy = how many predictions match the ground truth\n",
    "  return jnp.mean(jnp.argmax(predictions, axis=-1) == batch['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsukw533rbDL"
   },
   "source": [
    "### Define training step (parameters update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQ_zigsOr9lF"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def flax_train_step(state, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grads = jax.grad(flax_loss_fn)(state.params, batch)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMJTu6qBsxDY"
   },
   "source": [
    "### Initialize the model and the optimiser (train state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gb-zKQgCs9au"
   },
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRHuZrFfs3hI"
   },
   "outputs": [],
   "source": [
    "# We define a new (higher) learning rate for SGD and momentum\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VWuXzS4s5c5"
   },
   "outputs": [],
   "source": [
    "state = create_train_state(init_rng, learning_rate, momentum)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAML32ZCRgsw"
   },
   "outputs": [],
   "source": [
    "# Compare the number of params in this CNN to the haiku MLP \n",
    "print('Total number of parameters %d' % get_num_params(state.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2wmsbVXr3A7"
   },
   "source": [
    "### Accuracy of the untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zw50IuwQr3HK"
   },
   "outputs": [],
   "source": [
    "# Run accuracy on the test dataset\n",
    "test_accuracy = flax_accuracy(state.params, next(iter(test_eval_dataset)))\n",
    "print('Test accuracy %f '% test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwJh6L-4lrHD"
   },
   "source": [
    "### Run training steps in a loop. We also run evaluation periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_9IBXoVwFZJ"
   },
   "outputs": [],
   "source": [
    "# Train/eval loop.\n",
    "print(\"Getting data iterators..\")\n",
    "train_ds_iterator = iter(train_dataset)\n",
    "train_eval_ds_iterator = iter(train_eval_dataset)\n",
    "test_eval_ds_iterator = iter(test_eval_dataset)\n",
    "\n",
    "print(\"Training..\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  for step in range(TRAIN_NUM_EXAMPLES // TRAIN_BATCH_SIZE):\n",
    "    # Do SGD on a batch of training examples.\n",
    "    state = flax_train_step(state, next(train_ds_iterator))\n",
    "  # Periodically evaluate classification accuracy on train & test sets.\n",
    "  train_accuracy = flax_accuracy(state.params, next(train_eval_ds_iterator))\n",
    "  test_accuracy = flax_accuracy(state.params, next(test_eval_ds_iterator))\n",
    "  train_accuracy, test_accuracy = jax.device_get(\n",
    "      (train_accuracy, test_accuracy))\n",
    "  print('epoch %d Train / Test accuracy: %f / %f' % (epoch+1, train_accuracy, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJRviYOTwGVa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "intro-tutorial.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
