{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gianscarpe/eeml_summer_school_2021/blob/master/notebooks/rl_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULdrhOaVbsdO",
    "tags": []
   },
   "source": [
    "# RL Tutorial \n",
    "\n",
    "Original tutorial:\n",
    "\n",
    "Contact us at feryal@google.com & gcomanici@google.com for any questions/comments :)\n",
    "\n",
    "Special thanks to Anita Gergely and Bobak Shahriari.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qjbh0OWXqOMO",
    "tags": []
   },
   "source": [
    "## Summer school infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrh9Mj7yqOMP"
   },
   "source": [
    "Link: https://www.eeml.eu/\n",
    "\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/gianscarpe/eeml_summer_school_2021/master/notebooks/images/calendar.png\" width=\"500\" /></center>\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/gianscarpe/eeml_summer_school_2021/master/notebooks/images/people.png\" width=\"500\" /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installations  { form-width: \"30%\" }\n",
    "\n",
    "!git clone https://github.com/gianscarpe/eeml_summer_school_2021\n",
    "%cd eeml_summer_school_2021\n",
    "!pip install dm-acme\n",
    "!pip install dm-acme[reverb]\n",
    "!pip install dm-acme[jax]\n",
    "!pip install dm-acme[tf]\n",
    "!pip install dm-acme[envs]\n",
    "!pip install dm-env\n",
    "!pip install dm-haiku\n",
    "!apt install -y xvfb ffmpeg\n",
    "!pip install imageio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5Zen5LxqOMQ"
   },
   "source": [
    "# JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTnFWM1SqOMR"
   },
   "source": [
    "\n",
    "[JAX](https://jax.readthedocs.io/en/latest/jax.html) allows NumPy-like code to execute on CPU, or accelerators like GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n",
    "\n",
    "- JAX automatically differentiates python code and NumPy code (with [Autograd](https://github.com/hips/autograd))\n",
    "- uses [XLA](https://www.tensorflow.org/xla) to compile and run NumPy code efficiently on accelerators\n",
    "\n",
    "This makes JAX a great tool for high-performance numerical computing and machine learning research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zlry3BscqOMS"
   },
   "source": [
    "**Key Concepts:**\n",
    "\n",
    "* JAX provides a NumPy-inspired interface for convenience.\n",
    "* Through duck-typing, JAX arrays can often be used as drop-in replacements of NumPy arrays.\n",
    "* Unlike NumPy arrays, JAX arrays are always immutable.\n",
    "\n",
    "JAX has a functional interface, that is, all functions are pure.\n",
    "\n",
    "Various neural network libraries have been built on top of JAX to enable fast research and provide more familiar object oriented interfaces. We will see two of these below: haiku and flax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcoTIwXxqOMS"
   },
   "source": [
    "### JAX and random number generators\n",
    "Unlike many ML frameworks, JAX does not hide the pseudo-random number generator state. You need to generate explicitely a random key, and pass it to the operations that work with random numbers (e.g. initialising a model, dropout etc). A call to a random function with the same key does not change the state of the generator. This has to be done explicitely with `split()` or `next_rng_key()` in `haiku`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkW8gcq9qOMT",
    "outputId": "5ac12d91-feaf-4517-a0f1-add58353a31e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "key = random.PRNGKey(0)\n",
    "x1 = random.normal(key, (3,))\n",
    "print(x1)\n",
    "x2 = random.normal(key, (3,))\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42fyA8ijqOMW",
    "outputId": "b5490c7c-24a0-4e9d-fe66-0787895dee5b"
   },
   "outputs": [],
   "source": [
    "# Let's split the key to be able to generate different random values\n",
    "key, new_key = random.split(key)\n",
    "x1 = random.normal(key, (3,))\n",
    "print (x1)\n",
    "x2 = random.normal(new_key, (3,))\n",
    "print (x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KplapgOUqOMX"
   },
   "source": [
    "### JAX program transformations with examples \n",
    "* `jit` (just-in-time compilation) -- speeds up your code by running all the ops inside the jit-ed function as a *fused* op; it compiles the function when it's called the first time and uses the compiled (optimised) version from the second call onwards.\n",
    "* `grad` -- returns derivatives of function with respect to the model weights passed as parameters\n",
    "* `vmap` -- automatic batching; returns a new function that can apply the original (per-sample) function to a batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgzN4mLSqOMY",
    "outputId": "2dbf3f5c-4ab1-4cf7-a551-e9f021f461f5"
   },
   "outputs": [],
   "source": [
    "from jax import grad, jit\n",
    "# Let's use jit to speed up a function\n",
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "# execute the function without jit\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()   # block_until_ready is needed as jax, by default, runs operations asyncronously\n",
    "\n",
    "# Execute the function with jit and compare timing with above -- it should be much faster\n",
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyiMZvY9qOMZ"
   },
   "outputs": [],
   "source": [
    "# Let's use grad to compute gradient of a simple function\n",
    "def simple_fun(x):\n",
    "  return jnp.sin(x) / x\n",
    "\n",
    "# Get the gradient of simple_fun with respect to x\n",
    "grad_simple_fun = grad(simple_fun)\n",
    "\n",
    "# We can also get higher order derivatives, e.g. Hessian\n",
    "grad_grad_simple_fun = grad(grad(simple_fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4PkkeCOqOMZ",
    "outputId": "1bfa89a1-c78e-4fe2-cd99-f64db0f5f330"
   },
   "outputs": [],
   "source": [
    "# Let's plot the result\n",
    "import matplotlib.pyplot as plt\n",
    "x_range = jnp.arange(-8, 8, .1)\n",
    "plt.plot(x_range, simple_fun(x_range), 'b')\n",
    "plt.plot(x_range, [grad_simple_fun(xi) for xi in x_range], 'r')\n",
    "plt.plot(x_range, [grad_grad_simple_fun(xi) for xi in x_range], '--g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4IHd_8hqOMa",
    "outputId": "bd16be56-ff54-4bd9-ab98-1fa3e923aa09"
   },
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "# Let's see how vmap can be used to vectorize computations efficiently\n",
    "# In the example above, we can use vmap instead of loop to compute gradients\n",
    "\n",
    "grad_vect_simple_fun = vmap(grad_simple_fun)(x_range)\n",
    "\n",
    "# plot again and check that the gradients are identical \n",
    "plt.plot(x_range, simple_fun(x_range), 'b')\n",
    "plt.plot(x_range, [grad_simple_fun(xi) for xi in x_range], 'r')\n",
    "plt.plot(x_range, grad_vect_simple_fun, 'oc', mfc='none')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8n2T5Ff5qOMb",
    "outputId": "a81025ed-1868-4347-b5e5-913e6e3e1d00"
   },
   "outputs": [],
   "source": [
    "# Let's time them!\n",
    "\n",
    "# naive batching\n",
    "def naively_batched(x):\n",
    "  return jnp.stack([grad_simple_fun(xi) for xi in x])\n",
    "\n",
    "# manual batching with jit\n",
    "@jit\n",
    "def manual_batched(x):\n",
    "  return jnp.stack([grad_simple_fun(xi) for xi in x])\n",
    "\n",
    "# Batching using vmap and jit\n",
    "@jit\n",
    "def vmap_batched(x):\n",
    "  return vmap(grad_simple_fun)(x)\n",
    "\n",
    "print ('Naively batched')\n",
    "%timeit naively_batched(x_range).block_until_ready()\n",
    "print ('jit batched')\n",
    "%timeit manual_batched(x_range).block_until_ready()\n",
    "print ('With jit vmap')\n",
    "%timeit vmap_batched(x_range).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjJGfjKNqOMb"
   },
   "source": [
    "### Unofortunately for Google, pytorch is adding the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj66To1_qOMc"
   },
   "outputs": [],
   "source": [
    "!pip3 install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu111/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnodAIGJqOMc",
    "outputId": "ef9d3721-50d0-42f5-a80a-f3669f1ba172"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6raHC1GLqOMc"
   },
   "outputs": [],
   "source": [
    "def simple_fun(x):\n",
    "  return torch.sin(x) / x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pq3WnoeSqOMd",
    "outputId": "8839729e-32d3-4505-f0a9-ef2fcc8645cf"
   },
   "outputs": [],
   "source": [
    "x_range = torch.arange(-8, 8, .1)\n",
    "x_range.requires_grad_(True)\n",
    "batched_simplefun = torch.vmap(simple_fun)\n",
    "%timeit batched_simplefun(x_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffeeXVm4AuZ6",
    "tags": []
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT8rkxTUAZzL"
   },
   "source": [
    "The agent interacts with the environment in a loop corresponding to the following diagram. The environment defines a set of <font color='blue'>**actions**</font>  that an agent can take.  The agent takes an action informed by the <font color='red'>**observations**</font> it recieves, and will get a <font color='green'>**reward**</font> from the environment after each action. The goal in RL is to find an agent whose actions maximize the total accumulation of rewards obtained from the environment. \n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1sVOD2Ux5F_1Yq3KjyLOKFjFm2WRNTbIH\" width=\"500\" /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaJxoatMhJ71",
    "tags": []
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovuCuHCC78Zu"
   },
   "source": [
    "### Install required libraries\n",
    "\n",
    "1. [Acme](https://github.com/deepmind/acme) is a library of reinforcement learning (RL) agents and agent building blocks. Acme strives to expose simple, efficient, and readable agents, that serve both as reference implementations of popular algorithms and as strong baselines, while still providing enough flexibility to do novel research. The design of Acme also attempts to provide multiple points of entry to the RL problem at differing levels of complexity.\n",
    "\n",
    "\n",
    "2. [Haiku](https://github.com/deepmind/dm-haiku) is a simple neural network library for JAX developed by some of the authors of Sonnet, a neural network library for TensorFlow.\n",
    "\n",
    "3. [dm_env](https://github.com/deepmind/dm_env): DeepMind Environment API, which will be covered in more details in the [Environment subsection](https://colab.research.google.com/drive/1oKyyhOFAFSBTpVnmuOm9HXh5D5ekqhh5#scrollTo=I6KuVGSk4uc9) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g26p7OdbqOMe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KH3O0zcXUeun"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-H2d6UZi7Sf",
    "tags": []
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "HJ74Id-8MERq"
   },
   "outputs": [],
   "source": [
    "#@title Imports  { form-width: \"30%\" }\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython\n",
    "\n",
    "import acme\n",
    "from acme import environment_loop\n",
    "from acme import datasets\n",
    "from acme import specs\n",
    "from acme import wrappers\n",
    "from acme.wrappers import gym_wrapper\n",
    "from acme.agents.jax import dqn\n",
    "from acme.adders import reverb as adders\n",
    "from acme.utils import counting\n",
    "from acme.utils import loggers\n",
    "import base64\n",
    "import collections\n",
    "from collections import namedtuple\n",
    "import dm_env\n",
    "import enum\n",
    "import functools\n",
    "import gym\n",
    "import haiku as hk\n",
    "import io\n",
    "import imageio\n",
    "import itertools\n",
    "import jax\n",
    "from jax import tree_util\n",
    "# from jax.experimental import optix\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import reverb\n",
    "import rlax\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=1)\n",
    "\n",
    "plt.style.use('seaborn-notebook')\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeGPIOMkUTEn",
    "tags": []
   },
   "source": [
    "# RL Lab - Part 0: Environment & Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVbEdFOLqOMf",
    "tags": []
   },
   "source": [
    "## Notebook stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uJjEaRvuqOMg"
   },
   "outputs": [],
   "source": [
    "#@title Evaluation loop { form-width: \"30%\" }\n",
    "\n",
    "def display_video(frames, filename='temp.mp4', frame_repeat=1):\n",
    "  \"\"\"Save and display video.\"\"\"\n",
    "  # Write video\n",
    "  with imageio.get_writer(filename, fps=60) as video:\n",
    "    for frame in frames:\n",
    "      for _ in range(frame_repeat):\n",
    "        video.append_data(frame)\n",
    "  # Read video and display the video\n",
    "  video = open(filename, 'rb').read()\n",
    "  b64_video = base64.b64encode(video)\n",
    "  video_tag = ('<video  width=\"320\" height=\"240\" controls alt=\"test\" '\n",
    "               'src=\"data:video/mp4;base64,{0}\">').format(b64_video.decode())\n",
    "  return IPython.display.HTML(video_tag)\n",
    "\n",
    "#@title Helper functions for visualisation  { form-width: \"30%\" }\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "\n",
    "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_state_value(action_values, epsilon=0.1):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(\"$v(s)$\")\n",
    "\n",
    "def plot_action_values(action_values, epsilon=0.1):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(8, 8))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  dif = vmax - vmin\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    \n",
    "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(\"$v(s)$\")\n",
    "      \n",
    "\n",
    "def smooth(x, window=10):\n",
    "  return x[:window*(len(x)//window)].reshape(len(x)//window, window).mean(axis=1)\n",
    "  \n",
    "def plot_stats(stats, window=10):\n",
    "  plt.figure(figsize=(16,4))\n",
    "  plt.subplot(121)\n",
    "  xline = range(0, len(stats.episode_lengths), window)\n",
    "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\n",
    "  plt.ylabel('Episode Length')\n",
    "  plt.xlabel('Episode Count')\n",
    "  plt.subplot(122)\n",
    "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\n",
    "  plt.ylabel('Episode Return')\n",
    "  plt.xlabel('Episode Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6KuVGSk4uc9",
    "tags": []
   },
   "source": [
    "## MDP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhZwB__DPcyM"
   },
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1qBjh_PRdZ4GWTDqB9pmjLEOlUAsOfrZi\" width=\"500\" />\n",
    "\n",
    "### Markovian Decision Process\n",
    "\n",
    "Main components: set of **states**, set of **actions**, set of **rewards**, and dynamics of the environment\n",
    "$$ \\text{MDP} = ( \\mathcal{S}, \\mathcal{A}, \\mathcal{R}, p)$$\n",
    "\n",
    "Dynamics is expressed as a probability function of going into a certain state $s'$ while receiving reward $r$\n",
    "\n",
    "$$p(s', r|s, a) = Pr \\{S_t =s', R_t=r|S_{t-1} =s, A_{t-1}=a\\}$$\n",
    "We can rewrite this as\n",
    "$$p(s'|s, a) = Pr \\{S_t =s'|S_{t-1} =s, A_{t-1}=a\\} = \\sum_{r \\in \\mathcal{R}} p(s', r|s, a)$$\n",
    "A (finite) MCD episode is\n",
    "$$S_0, A_0, R_1, S_1, A_1, R_1, ... $$\n",
    "\n",
    "### Reward hypothesis:\n",
    "That all of what we mean by goals and purposes can be well thought of as\n",
    "the maximization of the **expected value of the cumulative sum of a received\n",
    "scalar signal (called rewards)**\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=0} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "where $\\gamma$ (discount rate) between 0 and 1\n",
    "\n",
    "Note that discounted returns are related\n",
    "$$G_t = R_{t+1} + \\gamma G_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "inIAhwLKuHKr",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'school_lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_91135/3285969202.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Gridworld Implementation { form-width: \"30%\" }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mschool_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_gridworld_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObservationType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'school_lib'"
     ]
    }
   ],
   "source": [
    "#@title Gridworld Implementation { form-width: \"30%\" }\n",
    "\n",
    "from school_lib.world import build_gridworld_task, ObservationType, setup_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZizdE9SQS-cN"
   },
   "source": [
    "\n",
    "We will use two distinct tabular GridWorlds:\n",
    "* `simple` where the goal is at the bottom left of the grid, little navigation required.\n",
    "* `obstacle` where the goal is behind an obstacle to avoid.\n",
    "\n",
    "You can visualize the grid worlds by running the cell below. \n",
    "\n",
    "Note that `S` indicates the start state and `G` indicates the goal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "7Xdnh3Odc63Q"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_gridworld_task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_91135/1999803573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Instantiate two tabular environments, a simple task, and one that involves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# the avoidance of an obstacle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m simple_grid = build_gridworld_task(\n\u001b[0m\u001b[1;32m      6\u001b[0m     task='simple', observation_type=ObservationType.GRID)\n\u001b[1;32m      7\u001b[0m obstacle_grid = build_gridworld_task(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_gridworld_task' is not defined"
     ]
    }
   ],
   "source": [
    "# @title Visualise gridworlds { form-width: \"30%\" }\n",
    "\n",
    "# Instantiate two tabular environments, a simple task, and one that involves\n",
    "# the avoidance of an obstacle.\n",
    "simple_grid = build_gridworld_task(\n",
    "    task='simple', observation_type=ObservationType.GRID)\n",
    "obstacle_grid = build_gridworld_task(\n",
    "    task='obstacle', observation_type=ObservationType.GRID)\n",
    "\n",
    "# Plot them.\n",
    "simple_grid.plot_grid()\n",
    "plt.title('Simple')\n",
    "\n",
    "obstacle_grid.plot_grid()\n",
    "plt.title('Obstacle');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqRb9fpdpe55"
   },
   "outputs": [],
   "source": [
    "simple_grid.plot_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTsiWgDSCL7C"
   },
   "source": [
    "\n",
    "In this environment, the agent has four possible  <font color='blue'>**Actions**</font>: `up`, `right`, `down`, and `left`.  <font color='green'>**Reward**</font> is `-5` for bumping into a wall, `+10` for reaching the goal, and `0` otherwise. The episode ends when the agent reaches the goal, and otherwise continues. **Discount** on continuing steps, is $\\gamma = 0.9$. \n",
    "\n",
    "Before we start building an agent to interact with this environment, let's first look at the types of objects the environment either returns (e.g. observations) or consumes (e.g. actions). The `environment_spec` will show you the form of the *observations*, *rewards* and *discounts* that the environment exposes and the form of the *actions* that can be taken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmKop4FECVV6"
   },
   "outputs": [],
   "source": [
    "environment, environment_spec = setup_environment(simple_grid)\n",
    "\n",
    "print('actions:\\n', environment_spec.actions, '\\n')\n",
    "print('observations:\\n', environment_spec.observations, '\\n')\n",
    "print('rewards:\\n', environment_spec.rewards, '\\n')\n",
    "print('discounts:\\n', environment_spec.discounts, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VVTmep2UK6U"
   },
   "source": [
    "\n",
    "We first set the environment to its initial location by calling the `reset` method which returns the first observation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHden9m9FNPK"
   },
   "outputs": [],
   "source": [
    "environment.reset()\n",
    "environment.plot_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXb7u9epFWnX"
   },
   "source": [
    "Now we want to take an action using the `step` method to interact with the environment which returns a `TimeStep` \n",
    "namedtuple with fields:\n",
    "\n",
    "```none\n",
    "step_type, reward, discount, observation\n",
    "``` \n",
    "\n",
    "We can then visualise the updated state of the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LY1eopIWFe95"
   },
   "outputs": [],
   "source": [
    "timestep = environment.step(1)\n",
    "environment.plot_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0YgLdsi3kXw",
    "tags": []
   },
   "source": [
    "## Agent\n",
    "\n",
    "We will be implementing Tabular & Function Approximation agents. Tabular agents are purely in Python while for Function Approximation agents, we will use JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MAYbEvtJ1kT"
   },
   "source": [
    "### Agent Implementation\n",
    "\n",
    "Each agent implements the following functions:\n",
    "\n",
    "\n",
    "\n",
    "> `__init__(self, number_of_actions, number_of_states, ...)`\n",
    "\n",
    "\n",
    "The constructor will provide the agent the number of actions and number of\n",
    "states.\n",
    "\n",
    "> `select_action(self, observation)`:\n",
    "\n",
    "This is the policy used by the actor to interact with the environment.\n",
    "\n",
    "> `observe_first(self, timestep)`:\n",
    "\n",
    "This function provides the agent with initial timestep in a given episode. Note\n",
    "that this is not the result of an action choice by the agent, hence it will only\n",
    "have `timestep.observation` set to a proper value.\n",
    "\n",
    "> `observe(self, action, next_timestep)`:\n",
    "\n",
    "This function provides the agent with the timestep that resulted from the given\n",
    "action choice. The timestep provides a `reward`, a `discount`, and an\n",
    "`observation`, all results of the previous action choice.\n",
    "\n",
    "> `update(self)`:\n",
    "\n",
    "This function commonly implements the update rules for the internal parameters of the agent (e.g. Q-values, network parameters for value or policy models, transition models).\n",
    "\n",
    "\n",
    "Note: `timestep.step_type` will be either `MID` or `LAST` and should be used to\n",
    "determine whether this is the last observation in the episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0lU-ybzz4Ng7"
   },
   "outputs": [],
   "source": [
    "#@title Example: Random Agent  { form-width: \"30%\" }\n",
    "\n",
    "# Uniform random policy\n",
    "def random_policy(q):\n",
    "  return np.random.randint(4)\n",
    "\n",
    "# (Do not worry about the details here, we will explain the Actor class below)\n",
    "class RandomAgent(acme.Actor):\n",
    "  def select_action(self, observation):\n",
    "    return random_policy(None)    \n",
    "    \n",
    "  def observe_first(self, timestep):\n",
    "    \"\"\"The agent is being notified that environment was reset.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def observe(self, action, next_timestep):\n",
    "    \"\"\"The agent is being notified of an environment step.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def update(self):    \n",
    "    \"\"\"Agent should update its parameters.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_U3ltgWqEPm",
    "tags": []
   },
   "source": [
    "## Experiment loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pSFDZPksEGpl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from school_lib.tools import evaluate, run_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XD9bXC3UCHd"
   },
   "source": [
    "Let's use these loops to run a Random Agent on our environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "oxjzoRO03jGH"
   },
   "outputs": [],
   "source": [
    "#@title Visualise random agent's behaviour { form-width: \"30%\" }\n",
    "\n",
    "# This is how the random policy moves around\n",
    "frames = evaluate(environment, RandomAgent(), evaluation_episodes=1)\n",
    "display_video(frames, frame_repeat=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPc0CrguF4GV",
    "tags": []
   },
   "source": [
    "# RL Lab - Part 1: Tabular Agents\n",
    "\n",
    "The first set of execises are based on the simpler case where the number of states is small enough for our agents to maintain a table of values for each state-action pair that it will ever encounter. For example, in the case of GridWorld, such a table may look something like this:\n",
    "\n",
    "\n",
    "| Q(s,a)  | up        | down      | left      | right     |\n",
    "| ------- | --------- | --------- | --------- | --------- |\n",
    "| $s_i$   | 0.7       | 0.0       | -0.5      | 0.32      |\n",
    "| $s_j$   | -1.0      | 0.1       | 0.2       | 0.3       |\n",
    "| $s_k$   | 0.4       | 0.15      | 0.6       | -0.23     |\n",
    "| $\\dots$ | $\\dots$   | $\\dots$   | $\\dots$   | $\\dots$   |\n",
    "\n",
    "\n",
    "\n",
    "In the exercises below, we will consider the case where the GridWorld has a fixed layout, and the goal is always at the same location, hence the state is fully determined by the location of the agent. As such, the <font color='red'>observation</font> from the environment is changed to be an integer corresponding to each one of approximately 50 locations on the grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAiZn23xICKh",
    "tags": []
   },
   "source": [
    "\n",
    "## 1.0: Overview\n",
    "\n",
    "We will cover three basic RL tabular algorithms:\n",
    "- Policy iteration\n",
    "- SARSA Agent\n",
    "- Q-learning Agent\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT7d9IIaDnUt",
    "tags": []
   },
   "source": [
    "### TD-error and Bellman equations. \n",
    "\n",
    "We need to learn a function that gives us the expected return when starting in sa and following $\\pi$:\n",
    "The tutorial is mainly focused on **value based methods**: agents are maintaining a value for all state-action pairs and use those estimates to choose actions that maximize that value (instead of maintaining a policy directly, like in policy gradient methods). \n",
    "\n",
    "\n",
    "Recall that efficient value estimations are based on the famous **_Bellman Optimallity Equation_**:\n",
    "\n",
    "$$ Q^\\pi(\\color{red}{s},\\color{blue}{a}) =  \\color{green}{r}(\\color{red}{s},\\color{blue}{a}) + \\gamma  \\sum_{\\color{red}{s'}\\in \\color{red}{\\mathcal{S}}} P(\\color{red}{s'} |\\color{red}{s},\\color{blue}{a}) V^\\pi(\\color{red}{s'}) $$\n",
    "\n",
    "where $V^\\pi$ is the expected $Q^\\pi$ value for a particular state, i.e. \n",
    "\n",
    "$$ V^\\pi(\\color{red}{s}) = \\mathop{\\mathbb{E}}_\\pi[G_t|S_t=s] = \\mathop{\\mathbb{E}}_\\pi[\\sum_{k=0} \\gamma^k R_{t+k+1} | S_t=s]$$\n",
    "\n",
    "Initially, we do not have good estimates of the $Q^\\pi$ values, so they do not satisfy this intuition. During training, we iteratively make updates to our $Q^\\pi$ estimates such that they are a bit closer to satisfying this relationship. Theory tells us that these updates will eventually converge to the optimal solution, i.e. our $Q^\\pi$ estimates will satisfy exactly the relationship:\n",
    "$$ Q^\\pi(\\color{red}{s},\\color{blue}{a}) = r(\\color{red}{s},\\color{blue}{a}) + \\gamma  Q^\\pi(\\color{red}{s'},\\color{blue}{a'})$$\n",
    "\n",
    "So how do we do this in practice? Well, if the condition is not already satisfied, that means \n",
    "\n",
    "$$ Q^\\pi(\\color{red}{s},\\color{blue}{a}) \\neq \\color{green}{r}(\\color{red}{s},\\color{blue}{a}) + \\gamma Q^\\pi(\\color{red}{s'},\\color{blue}{a'})$$\n",
    "\n",
    "This difference between what $Q^\\pi$ *currently is* and what we *want it to be* is called **TD-error**, which, in the long term, we hope will converge to 0.\n",
    "\n",
    "To make our updates, in every step will we want to compute the TD-error\n",
    "\n",
    "$$\\delta = \\color{green}{r}(\\color{red}{s},\\color{blue}{a}) + \\gamma Q^\\pi(\\color{red}{s'},\\color{blue}{a'}) - Q^\\pi(\\color{red}{s},\\color{blue}{a})$$\n",
    "\n",
    "Then we can use this TD-error to update $Q^\\pi$:\n",
    "\n",
    "$$Q^\\pi(\\color{red}{s},\\color{blue}{a}) \\mathrel{{+}{=}} \\color{green}{r}(\\color{red}{s},\\color{blue}{a}) + \\gamma Q^\\pi(\\color{red}{s'},\\color{blue}{a'}) - Q^\\pi(\\color{red}{s},\\color{blue}{a})$$\n",
    "\n",
    "We see two immediate questions here.\n",
    " * How do we choose $\\color{blue}{a}$? $\\rightarrow$ behaviour policy $\\pi_b$\n",
    " * How do we define the \"next\" action $\\color{blue}{a'}$? $\\rightarrow$ evaluated policy $\\pi_e$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhHsnLcFID1u",
    "tags": []
   },
   "source": [
    "## 1.1: Policy iteration\n",
    "\n",
    "The first RL learning algorithm we will explore is **policy iteration**, which is repeating (1) Policy Evaluation and (2) Greedy Improvement until convergence.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1lP2dFEXCBgYW744S3Lr3zMzVfOEYowdJ\" width=\"300\" /></center>\n",
    "\n",
    "For this exercise, we'll show you how to implement the \"first 2 arrows\", we will not repeat these steps to convergence yet.\n",
    "\n",
    "### 1. Policy Evaluation \n",
    "\n",
    "The purpose here is to evaluate a given policy $\\pi_e$:\n",
    "\n",
    "Compute the value function associated with following/employing this policy in a given MDP.\n",
    "\n",
    "$$ Q^{\\pi_e}(\\color{red}{s},\\color{blue}{a}) = \\mathbb{E}_{\\tau \\sim P^{\\pi_b}} \\left[ \\sum_t \\gamma^t \\color{green}{R_t}| s_0=\\color{red}s,a=\\color{blue}{a_0} \\right]$$\n",
    "\n",
    "where $\\tau = \\{\\color{red}{s_0}, \\color{blue}{a_0}, \\color{green}{r_0}, \\color{red}{s_1}, \\color{blue}{a_1}, \\color{green}{r_1}, \\cdots \\}$.\n",
    "\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "**Initialize** $Q(\\color{red}{s}, \\color{blue}{a})$ for all $\\color{red}{s}$ ∈ $\\mathcal{\\color{red}S}$ and $\\color{blue}a$ ∈ $\\mathcal{\\color{blue}A}(\\color{red}s)$\n",
    "\n",
    "**Loop forever**:\n",
    "\n",
    "1. $\\color{red}{s} \\gets{}$current (nonterminal) state\n",
    " \n",
    "2. $\\color{blue}{a} \\gets{} \\text{behaviour_policy }\\pi_b(\\color{red}s)$\n",
    " \n",
    "3. Take action $\\color{blue}{a}$; observe resulting reward $\\color{green}{r}$, discount $\\gamma$, and state, $\\color{red}{s'}$\n",
    "\n",
    "4. Compute TD-error: $\\delta = \\color{green}R + \\gamma Q(\\color{red}{s'}, \\underbrace{\\pi_e(\\color{red}{s'}}_{\\color{blue}{a'}})) − Q(\\color{red}s, \\color{blue}a)$\n",
    "\n",
    "4. Update Q-value with a small $\\alpha$ step: $Q(\\color{red}s, \\color{blue}a) \\gets Q(\\color{red}s, \\color{blue}a) + \\alpha \\delta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLdAIdA5qOMp"
   },
   "source": [
    "### 2. Greedy Policy Improvement\n",
    "\n",
    "Once a good approximation to the Q-value of a policy is obtained, we can improve this policy by simply changing action selection towards those that are evaluated higher. \n",
    "\n",
    "$$ \\pi_{greedy} (\\color{blue}a | \\color{red}s) = \\arg\\max_\\color{blue}a Q^{\\pi_e}(\\color{red}s,\\color{blue}a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqrSos8dDPFb"
   },
   "source": [
    "### Create a policy evaluation agent\n",
    "\n",
    "An ACME `Actor` is the part of our framework that directly interacts with an environment by generating actions. Here we borrow a figure from Acme to show how this interaction occurs:\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1T7FTpA9RgDYFkciDFZK4brNyURZN_ZGp\" width=\"500\" /></center>\n",
    "\n",
    "While you can always write your own actor, we also provide a number of useful premade versions. \n",
    "\n",
    "Tabular agents implement a function `q_values()` returning a matrix of Q values\n",
    "of shape: (`number_of_states`, `number_of_actions`)\n",
    "\n",
    "In this section, we will implement a `PolicyEvalAgent` as an ACME actor: given an `evaluation_policy` and a `behaviour_policy`, it will use the `behaviour_policy` to choose actions, and it will use the corresponding trajectory data to evaluate the `evaluation_policy` (i.e. compute the Q-values as if you were following the `evaluation_policy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BXMd87q0JVdr"
   },
   "outputs": [],
   "source": [
    "#@title Policy Evaluation Agent{ form-width: \"30%\" }\n",
    "class PolicyEvalAgent(acme.Actor):\n",
    "\n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, \n",
    "      evaluated_policy, \n",
    "      behaviour_policy=random_policy, \n",
    "      step_size=0.1):\n",
    "    self._state = None\n",
    "    self._number_of_states = number_of_states\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    self._evaluated_policy = evaluated_policy\n",
    "    \n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    self._action = None\n",
    "    self._next_state = None\n",
    "\n",
    "  @property\n",
    "  def q_values(self):\n",
    "    return self._q\n",
    "\n",
    "  def select_action(self, observation):\n",
    "    return self._behaviour_policy(self._q[observation])\n",
    "    \n",
    "  def observe_first(self, timestep):\n",
    "    self._state = timestep.observation\n",
    "\n",
    "  def observe(self, action, next_timestep):\n",
    "    s = self._state\n",
    "    a = action\n",
    "    r = next_timestep.reward\n",
    "    g = next_timestep.discount\n",
    "    next_s = next_timestep.observation\n",
    "    \n",
    "    # Compute TD-Error.\n",
    "    self._action = a\n",
    "    self._next_state = next_s\n",
    "    \n",
    "    next_a = self._evaluated_policy(self._q[next_s])\n",
    "    self._td_error = r + g * self._q[next_s, next_a] - self._q[s, a]\n",
    "    \n",
    "    \n",
    "  def update(self):\n",
    "    # Q-value table update.\n",
    "    s = self._state\n",
    "    a = self._action\n",
    "    self._q[s, a] += self._step_size * self._td_error\n",
    "    self._state = self._next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsuM-9sEKeT1"
   },
   "source": [
    "We will first see how this works on the `simple` GridWorld task.\n",
    "\n",
    "**Task 1**: Run the policy evaluation agent, evaluating the uniformly random policy on the `simple` task.\n",
    "\n",
    "Try different number of training steps, e.g. $\\texttt{num_steps} = 1e3, 1e5$. \n",
    "\n",
    "Visualise the resulting value functions $Q(\\color{red}s,\\color{blue}a)$.\n",
    "The plotting function is provided for you and it takes in a table of q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "YMumNsJIKhn_"
   },
   "outputs": [],
   "source": [
    "num_steps = 1e3  #@param {type:\"number\"}\n",
    "\n",
    "# environment\n",
    "grid = build_gridworld_task(task='simple')\n",
    "environment, environment_spec = setup_environment(grid)\n",
    "\n",
    "# agent \n",
    "agent = PolicyEvalAgent(\n",
    "    number_of_states=environment_spec.observations.num_values, \n",
    "    number_of_actions=environment_spec.actions.num_values, \n",
    "    evaluated_policy=random_policy,\n",
    "    behaviour_policy=random_policy,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "returns = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
    "\n",
    "# get the q-values\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualize value functions\n",
    "print('AFTER {} STEPS ...'.format(num_steps))\n",
    "plot_action_values(q, epsilon=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EfsRbSUN0B1"
   },
   "source": [
    "### Greedy Policy Improvement\n",
    "\n",
    "**Task 2**: Compute and Visualise the greedy policy based on the above evaluation, at the end of training.\n",
    "\n",
    "\n",
    "$$\\pi_{greedy} (\\color{blue}a|\\color{red}s) = \\arg\\max_\\color{blue}a Q^{\\pi_e}(\\color{red}s,\\color{blue}a)$$\n",
    "\n",
    "**Q:** What do you observe? How does it compare to the behaviour policy we started from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iTMx-QHU1f_j"
   },
   "outputs": [],
   "source": [
    "# @title Greedy policy\n",
    "def greedy(q_values):\n",
    "  return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bCcnLnvLOBYZ"
   },
   "outputs": [],
   "source": [
    "# @title Visualize the policy on `simple` { form-width: \"30%\" }\n",
    "\n",
    "# Do here whatever works for you, but you should be able to see what the agent\n",
    "# would do at each step/state.\n",
    "\n",
    "pi = np.zeros(grid._layout_dims, dtype=np.int32)\n",
    "for i in range(grid._layout_dims[0]):\n",
    "  for j in range(grid._layout_dims[1]):\n",
    "    pi[i, j] = greedy(q[i, j])\n",
    "    \n",
    "grid.plot_policy(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTczeMrWqOMr"
   },
   "outputs": [],
   "source": [
    "frames = evaluate(environment, agent, evaluation_episodes=1)\n",
    "display_video(frames, frame_repeat=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3fDpu9eOUm0"
   },
   "source": [
    "**Task 3**: Now try on the harder `obstacle` task and visualise the resulting value functions and the greedy policy on top of these values at the end of training.\n",
    "\n",
    "**Q:** What do you observe? \n",
    "- How does this policy compare with the optimal one?\n",
    "- Try running the training process longer -- what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Y4dbI0DLOeqC",
    "outputId": "e63224c3-9ce5-4034-9763-48a0940b9c2c"
   },
   "outputs": [],
   "source": [
    "num_steps = 1e5 #@param {type:\"number\"}\n",
    "\n",
    "# environment\n",
    "grid = build_gridworld_task(task='obstacle')\n",
    "environment, environment_spec = setup_environment(grid)\n",
    "\n",
    "# agent \n",
    "agent = PolicyEvalAgent(\n",
    "    number_of_states=environment_spec.observations.num_values, \n",
    "    number_of_actions=environment_spec.actions.num_values, \n",
    "    evaluated_policy=random_policy,\n",
    "    behaviour_policy=random_policy,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "returns = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
    "\n",
    "# get the q-values\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualize value functions\n",
    "print('AFTER {} STEPS ...'.format(num_steps))\n",
    "plot_action_values(q, epsilon=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "D4KxWhW8PCD9"
   },
   "outputs": [],
   "source": [
    "# @title Visualise the greedy policy on `obstacle` { form-width: \"30%\" }\n",
    "grid.plot_greedy_policy(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcrhrNnIr3kX",
    "tags": []
   },
   "source": [
    "## 1.2 On-policy control: SARSA Agent\n",
    "In this section, we are focusing on control RL algorithms, which perform the evaluation and improvement of the policy synchronously. That is, the policy that is being evaluated improves as the agent is using it to interact with the environent.\n",
    "\n",
    "\n",
    "The first algorithm we are going to be looking at is SARSA. This is an **on-policy algorithm** -- i.e: the data collection is done by leveraging the policy we're trying to optimize (and not just another fixed behaviour policy). \n",
    "\n",
    "As discussed during lectures, a greedy policy with respect to a given estimate of $Q^\\pi$ fails to explore the environment as needed; we will use instead an $\\epsilon$-greedy policy WRT $Q^\\pi$.\n",
    "\n",
    "### SARSA Algorithm\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/gianscarpe/eeml_summer_school_2021/master/notebooks/images/sarsa.png\" width=\"500\" /></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XWqlIWbwN7Mk"
   },
   "outputs": [],
   "source": [
    "# @title Epilson-greedy policy { form-width: \"30%\" }\n",
    "def epsilon_greedy(q_values, epsilon=0.1):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JtlH1tU7sCEm"
   },
   "outputs": [],
   "source": [
    "#@title SARSA Agent { form-width: \"30%\" }\n",
    "class SarsaAgent(acme.Actor):\n",
    "\n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, epsilon, step_size=0.1): \n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    self._number_of_states = number_of_states\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "    self._epsilon = epsilon\n",
    "    self._state = None\n",
    "    self._action = None\n",
    "    self._next_state = None\n",
    "    \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    return self._q\n",
    "\n",
    "  def select_action(self, observation):\n",
    "    return epsilon_greedy(self._q[observation], self._epsilon)\n",
    "    \n",
    "  def observe_first(self, timestep):\n",
    "    self._state = timestep.observation\n",
    "\n",
    "  def observe(self, action, next_timestep):\n",
    "    s = self._state\n",
    "    a = action\n",
    "    r = next_timestep.reward\n",
    "    g = next_timestep.discount\n",
    "    next_s = next_timestep.observation\n",
    "    next_a = epsilon_greedy(self._q[next_s], self._epsilon)\n",
    "    \n",
    "    # Online Q-value update\n",
    "    self._action = a\n",
    "    self._next_state = next_s\n",
    "    self._td_error = r + g * self._q[next_s, next_a] - self._q[s, a]\n",
    "\n",
    "  def update(self):\n",
    "    s = self._state\n",
    "    a = self._action\n",
    "    self._q[s, a] += self._step_size * self._td_error\n",
    "    self._state = self._next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8eBOcXZu1fM"
   },
   "source": [
    "### **Task**: Run your SARSA agent on the `obstacle` environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "xKYEB2d2uGaa"
   },
   "outputs": [],
   "source": [
    "num_steps = 1e5 #@param {type:\"number\"}\n",
    "\n",
    "# environment\n",
    "grid = build_gridworld_task(task='obstacle')\n",
    "environment, environment_spec = setup_environment(grid)\n",
    "\n",
    "# agent \n",
    "agent = SarsaAgent(\n",
    "    number_of_states=environment_spec.observations.num_values, \n",
    "    number_of_actions=environment_spec.actions.num_values, \n",
    "    epsilon=0.1,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "returns = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
    "\n",
    "# get the q-values\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualize value functions\n",
    "print('AFTER {} STEPS ...'.format(num_steps))\n",
    "plot_action_values(q, epsilon=1.)\n",
    "\n",
    "# visualise the greedy policy\n",
    "grid.plot_greedy_policy(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaRGna8RqOMu"
   },
   "source": [
    "### Convergence\n",
    "The convergence properties of the Sarsa algorithm depend on the nature of the policy’s\n",
    "dependence on Q. For example, one could use eps-greedy policies. Sarsa converges\n",
    "with probability 1 to an optimal policy and action-value function as long as all state—action\n",
    "pairs are visited an infinite number of times and the policy converges in the limit to\n",
    "the greedy policy (which can be arranged, for example, with eps-greedy policies by setting\n",
    "eps = 1 /t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFGX_zGcvb8D",
    "tags": []
   },
   "source": [
    "## 1.3 Off-policy control: Q-learning Agent\n",
    "\n",
    "Reminder: Q-learning is a very powerful and general algorithm, that enables control (figuring out the optimal policy/value function) both on and off-policy.\n",
    "\n",
    "**Initialize** $Q(\\color{red}{s}, \\color{blue}{a})$ for all $\\color{red}{s} \\in \\color{red}{\\mathcal{S}}$ and $\\color{blue}{a} \\in \\color{blue}{\\mathcal{A}}(\\color{red}{s})$\n",
    "\n",
    "**Loop forever**:\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/gianscarpe/eeml_summer_school_2021/master/notebooks/images/qlearning.png\" width=\"500\" /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fEZhJq_qOMu"
   },
   "source": [
    "### Question\n",
    "- ***Why is Q-learning considered an off-policy control method?***\n",
    "- ***Suppose action selection is greedy. Is Q-learning then exactly the same\n",
    "algorithm as Sarsa? Will they make exactly the same action selections and weight\n",
    "updates?***\n",
    "\n",
    "Anserws at https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions/blob/master/Chapter%206/Solutions_to_Reinforcement_Learning_by_Sutton_Chapter_6_rx.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ak1T5PNV8Pbk"
   },
   "outputs": [],
   "source": [
    "#@title Q-Learning Agent { form-width: \"30%\" }\n",
    "class QLearningAgent(acme.Actor):\n",
    "\n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, behaviour_policy, step_size=0.1): \n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    self._state = None\n",
    "    self._action = None\n",
    "    self._next_state = None\n",
    "    \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    return self._q\n",
    "\n",
    "  def select_action(self, observation):\n",
    "    return self._behaviour_policy(self._q[observation])\n",
    "    \n",
    "  def observe_first(self, timestep):\n",
    "    self._state = timestep.observation\n",
    "\n",
    "  def observe(self, action, next_timestep):\n",
    "    s = self._state\n",
    "    a = action\n",
    "    r = next_timestep.reward\n",
    "    g = next_timestep.discount\n",
    "    next_s = next_timestep.observation\n",
    "    \n",
    "    # Offline Q-value update\n",
    "    self._action = a\n",
    "    self._next_state = next_s\n",
    "    self._td_error = r + g * np.max(self._q[next_s]) - self._q[s, a]\n",
    "\n",
    "  def update(self):\n",
    "    s = self._state\n",
    "    a = self._action\n",
    "    self._q[s, a] += self._step_size * self._td_error\n",
    "    self._state = self._next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RqdV3rjwcAh",
    "tags": []
   },
   "source": [
    "### **Task 1**: Run your Q-learning agent on `obstacle`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LL4PgT-jwi3-"
   },
   "outputs": [],
   "source": [
    "epsilon = 1  #@param {type:\"number\"} \n",
    "num_steps = 1e5  #@param {type:\"number\"}\n",
    "\n",
    "# environment\n",
    "grid = build_gridworld_task(task='obstacle')\n",
    "environment, environment_spec = setup_environment(grid)\n",
    "\n",
    "# behavior policy\n",
    "behavior_policy = lambda qval: epsilon_greedy(qval, epsilon=epsilon)\n",
    "\n",
    "# agent\n",
    "agent = QLearningAgent(\n",
    "    number_of_states=environment_spec.observations.num_values,\n",
    "    number_of_actions=environment_spec.actions.num_values,\n",
    "    behaviour_policy=behavior_policy,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "returns = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
    "\n",
    "# get the q-values\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualize value functions\n",
    "print('AFTER {} STEPS ...'.format(num_steps))\n",
    "plot_action_values(q, epsilon=epsilon)\n",
    "\n",
    "# visualise the greedy policy\n",
    "grid.plot_greedy_policy(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lceLiRr0qOMv"
   },
   "outputs": [],
   "source": [
    "frames = evaluate(environment, agent, evaluation_episodes=1)\n",
    "display_video(frames, frame_repeat=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMk2ArG-weg_",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Task 2:** Experiment with different levels of 'greediness'\n",
    "* The default was $\\epsilon=1.$, what does this correspond to?\n",
    "* Try also $\\epsilon =0.1, 0.5$. What do you observe? Does the behaviour policy affect the training in any way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFig9STjM827",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1.4 Recap: Policy iteration vs. SARSA vs. Q-learning\n",
    "\n",
    "The table below captures the main differences between the three algorithms we just discussed.\n",
    "\n",
    "<table width=\"700\">\n",
    "  <colgroup>\n",
    "      <col span=\"1\" width=\"100\">\n",
    "      <col span=\"1\" width=\"190\">\n",
    "      <col span=\"1\" width=\"150\">\n",
    "      <col span=\"1\" width=\"400\">\n",
    "  </colgroup>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td height=\"20\"> <b> Algorithm </b> </td>\n",
    "      <td> <b> Behaviour policy </b> </td>\n",
    "      <td> <b> Evaluated policy </b> </td>\n",
    "      <td> <b> Update rule </b> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td height=\"50\"> Policy Iteration </td>\n",
    "      <td> any (e.g. random) --> off-policy </td>\n",
    "      <td> changes periodically </td>\n",
    "      <td> $Q(\\color{red}s, \\color{blue}a) \\gets Q(\\color{red}s, \\color{blue}a) + \\alpha (\\color{green}R + \\gamma Q(\\color{red}{s'}, \\underbrace{\\pi_e(\\color{red}{s'}}_{\\color{blue}{a'}})) − Q(\\color{red}s, \\color{blue}a))$  </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td height=\"50\"> SARSA </td>\n",
    "      <td> epsilon-greedy --> on-policy </td>\n",
    "      <td> changes after every step </td>\n",
    "      <td> $Q(\\color{red}s, \\color{blue}a) \\gets Q(\\color{red}s, \\color{blue}a) + \\alpha (\\color{green}R + \\gamma Q(\\color{red}{s'}, \\color{blue}{a'}) − Q(\\color{red}s, \\color{blue}a))$   </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td height=\"50\"> Q-learning </td>\n",
    "      <td> any (e.g. random) --> off-policy </td>\n",
    "      <td> changes after every step </td>\n",
    "      <td> $Q(\\color{red}{s}, \\color{blue}{a}) \\gets Q(\\color{red}{s}, \\color{blue}{a}) + \\alpha (\\color{green}{R} + \\gamma \\max_{\\color{blue}{a'}} Q(\\color{red}{s'}, \\color{blue}{a'}) − Q(\\color{red}{s}, \\color{blue}{a}))$   </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "$ $     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqg1n48y81ei",
    "tags": []
   },
   "source": [
    "## 1.5 Experience Replay\n",
    "\n",
    "Implement an agent that uses **Experience Replay** to learn action values, at each step:\n",
    "* select actions \n",
    "* accumulate all observed transitions *(s, a, r, s')* in the environment in a *replay buffer*,\n",
    "* apply an online Q-learning \n",
    "* apply multiple Q-learning updates based on transitions sampled from the *replay buffer* (in addition to the online updates).\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/gianscarpe/eeml_summer_school_2021/master/notebooks/images/dynaq.png\" width=\"500\" /></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "I6Lunsx1-kmf"
   },
   "outputs": [],
   "source": [
    "#@title Q-learning AGENT with a simple replay buffer { form-width: \"30%\" }\n",
    "class ReplayQLearningAgent(acme.Actor):\n",
    "\n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, behaviour_policy, \n",
    "      num_offline_updates=0, step_size=0.1): \n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    self._num_offline_updates = num_offline_updates\n",
    "    self._state = None\n",
    "    self._action = None\n",
    "    self._next_state = None\n",
    "    self._replay_buffer = []\n",
    "    \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    return self._q\n",
    "\n",
    "  def select_action(self, observation):\n",
    "    return self._behaviour_policy(self._q[observation])\n",
    "    \n",
    "  def observe_first(self, timestep):\n",
    "    self._state = timestep.observation\n",
    "    \n",
    "\n",
    "  def observe(self, action, next_timestep):\n",
    "    s = self._state\n",
    "    a = action\n",
    "    r = next_timestep.reward\n",
    "    g = next_timestep.discount\n",
    "    next_s = next_timestep.observation\n",
    "    \n",
    "    # Offline Q-value update\n",
    "    self._action = a\n",
    "    self._next_state = next_s\n",
    "    self._td_error = r + g * np.max(self._q[next_s]) - self._q[s, a]\n",
    "\n",
    "    if self._num_offline_updates > 0:\n",
    "      self._replay_buffer.append((s, a, r, g, next_s))\n",
    "\n",
    "  def update(self):\n",
    "    s = self._state\n",
    "    a = self._action\n",
    "    self._q[s, a] += self._step_size * self._td_error\n",
    "    self._state = self._next_state\n",
    "\n",
    "    # Offline Q-value update\n",
    "    if len(self._replay_buffer) > self._num_offline_updates:\n",
    "      for i in range(self._num_offline_updates):\n",
    "        idx = np.random.randint(0, len(self._replay_buffer))\n",
    "        s, a, r, g, next_s = self._replay_buffer[idx]\n",
    "        td_error = r + g * np.max(self._q[next_s]) - self._q[s, a]\n",
    "        self._q[s, a] += self._step_size * td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3J6CE2M_AdF",
    "tags": []
   },
   "source": [
    "### **Task 1**: Compare Q-learning with/without experience replay and random actions\n",
    "\n",
    "Use a small number of training steps (e.g. `num_steps = 1e3`) and vary `num_offline_updates` between `0` and `30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9yLCXKBH_F0j"
   },
   "outputs": [],
   "source": [
    "num_offline_updates = 30 #@param {type:\"integer\"}\n",
    "num_steps = 1e3 #@param {type:\"number\"}\n",
    "\n",
    "grid = build_gridworld_task(task='obstacle')\n",
    "environment, environment_spec = setup_environment(grid)\n",
    "\n",
    "agent = ReplayQLearningAgent(\n",
    "    number_of_states=environment_spec.observations.num_values,\n",
    "    number_of_actions=environment_spec.actions.num_values,\n",
    "    behaviour_policy=random_policy,\n",
    "    num_offline_updates=num_offline_updates,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "returns = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
    "\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)\n",
    "\n",
    "grid.plot_greedy_policy(q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deGdKEIKqOMx"
   },
   "outputs": [],
   "source": [
    "frames = evaluate(environment, agent, evaluation_episodes=1)\n",
    "display_video(frames, frame_repeat=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98jd9OiRqOMx"
   },
   "source": [
    "### Task 2: DYNAQ (replay and eps-greedy policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqHlGkNXqOMy"
   },
   "outputs": [],
   "source": [
    "epsilon = 1  #@param {type:\"number\"} \n",
    "num_steps = 1e5  #@param {type:\"number\"}\n",
    "num_offline_updates = 10 #@param {type:\"number\"}\n",
    "# environment\n",
    "grid = build_gridworld_task(task='obstacle')\n",
    "environment, environment_spec = setup_environment(grid)\n",
    "\n",
    "# behavior policy\n",
    "behavior_policy = lambda qval: epsilon_greedy(qval, epsilon=epsilon)\n",
    "\n",
    "agent = ReplayQLearningAgent(\n",
    "    number_of_states=environment_spec.observations.num_values,\n",
    "    number_of_actions=environment_spec.actions.num_values,\n",
    "    behaviour_policy=epsilon_greedy,\n",
    "    num_offline_updates=num_offline_updates,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "returns = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
    "\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)\n",
    "\n",
    "grid.plot_greedy_policy(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIic01qwqOMy"
   },
   "outputs": [],
   "source": [
    "frames = evaluate(environment, agent, evaluation_episodes=1)\n",
    "display_video(frames, frame_repeat=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB5LCIgNqOMz",
    "tags": []
   },
   "source": [
    "## 1.6 Monte-Carlo approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiWQF29-qOM1"
   },
   "source": [
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/gianscarpe/eeml_summer_school_2021/master/notebooks/images/mc.png\" width=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6TkcRZfqOM1"
   },
   "outputs": [],
   "source": [
    "# @title Epilson-greedy policy { form-width: \"30%\" }\n",
    "def epsilon_greedy(q_values, epsilon=0.1):\n",
    "  if epsilon < np.random.random():\n",
    "    return np.argmax(q_values)\n",
    "  else:\n",
    "    return np.random.randint(np.array(q_values).shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZ8ekYnkqOM1"
   },
   "outputs": [],
   "source": [
    "from school_lib.models.mc import MCAgentUpdateFirst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfaA7QQ9qOM1"
   },
   "outputs": [],
   "source": [
    "num_steps = 1e4 #@param {type:\"number\"}\n",
    "behavior_policy = lambda qval: epsilon_greedy(qval, epsilon=epsilon)\n",
    "grid = build_gridworld_task(task='obstacle')\n",
    "environment, environment_spec = setup_environment(grid)\n",
    "agent = MCAgentUpdateFirst(\n",
    "    number_of_states=environment_spec.observations.num_values,\n",
    "    number_of_actions=environment_spec.actions.num_values,\n",
    "    behaviour_policy= behavior_policy)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "returns = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\n",
    "\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)\n",
    "\n",
    "grid.plot_greedy_policy(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "np5rHKNqqOM2"
   },
   "outputs": [],
   "source": [
    "frames = evaluate(environment, agent, evaluation_episodes=1)\n",
    "display_video(frames, frame_repeat=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rkn2ud_0Pn2o",
    "tags": []
   },
   "source": [
    "# RL Lab - Part 2: Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxqnvCLoe3KU"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1oqIQNM_tMPmP8l38C_3yp5uUego3S8kV\" width=\"500\" />\n",
    "\n",
    "So far we only considered look-up tables. In all previous cases every state and action pair $(\\color{red}{s}, \\color{blue}{a})$, had an entry in our Q table. Again, this is possible in this environment as the number of states is a equal to the number of cells in the grid. But this is not scalable to situations where, say, the goal location changes or the obstacles are in different locations at every episode (consider how big the table should be in this situation?).\n",
    "\n",
    "As example (not covered in this tutorial) is ATARI from pixels, where the number of possible frames an agent can see is exponential in the number of pixels on the screen.\n",
    "\n",
    "<center><img width=\"200\" alt=\"portfolio_view\" src=\"https://miro.medium.com/max/1760/1*XyIpmXXAjbXerDzmGQL1yA.gif\"></center>\n",
    "\n",
    "But what we **really** want is just being able to *compute* the Q-value, when fed with a particular $(\\color{red}{s}, \\color{blue}{a})$ pair. So if we had a way to get a function to do this work instead of keeping a big table, we'd get around this problem.\n",
    "\n",
    "To address this, we can use **Function Approximation** as a way to generalize Q-values over some representation of the very large state space, and **train** them to output the values they should. In this section, we will explore Q-Learning with function approximation, which, although theoretically proven to diverge for some degenerate MDPs, can yield impressive results in very large environments. In particular, we will look at [Neural Fitted Q (NFQ) Iteration](http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTAYVPnaJN0t",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.1 NFQ agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-omtUOQCS8VI"
   },
   "source": [
    "[Neural Fitted Q Iteration](http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf) was one of the first papers to demonstrate how to leverage recent advances in Deep Learning to approximate the Q-value by a neural network $^1$.\n",
    "\n",
    "We represent $Q(\\color{red}s, \\color{blue}a)$ as a neural network $f()$. which given a vector $\\color{red}s$, will output a vector of Q-values for all possible actions $\\color{blue}a$.$^2$\n",
    "\n",
    "When introducing function approximations, and neural networks in particular, we need to have a loss to optimize. But looking back at the tabular setting above, you can see that we already have some notion of error: the **TD error**.\n",
    "\n",
    "By training our neural network to output values such that the *TD error is minimized*, we will also satisfy the Bellman Optimality Equation, which is a good sufficient condition to enforce, so that we may obtain an optimal policy.\n",
    "Thanks to automatic differentiation, we can just write the TD error as a loss (e.g. with a $L2$ loss, but others would work too), compute its gradient (which are now gradients with respect to individual parameters of the neural network) and slowly improve our Q-value approximation:\n",
    "\n",
    "$$ Loss = \\mathbb{E}\\left[ \\left( \\color{green}{r} + \\gamma \\max_\\color{blue}{a'} Q(\\color{red}{s'}, \\color{blue}{a'}) − Q(\\color{red}{s}, \\color{blue}{a})  \\right)^2\\right] $$\n",
    "\n",
    "\n",
    "NFQ builds on Q-learning, but if one were to update the Q-values online directly, the training can be unstable and very slow.\n",
    "Instead, NFQ uses a Replay buffer, similar to what you just implemented above, to update the Q-value in a batched setting.\n",
    "\n",
    "When it was introduced, it also was entirely off-policy (i.e. one would use a random policy to collect data), and is prone to unstability when applied to more complex environments (e.g. when the input are pixels or the tasks are longer and complicated).\n",
    "But it is a good stepping stone to the more complex agents used today. Here, we will look at a slightly different and modernised implementation of NFQ.\n",
    "\n",
    "<br />\n",
    "\n",
    "---\n",
    "\n",
    "<sub>*$^1$ if you read the NFQ paper, they use a \"control\" notation, where there is a \"cost to minimize\", instead of \"rewards to maximize\", so don't be surprised if signs/max/min do not correspond.* </sub>\n",
    "\n",
    "<sub>*$^2$ we could feed it $\\color{blue}a$ as well and ask $f$ for a single scalar value, but given we have a fixed number of actions and we usually need to take an $argmax$ over them, it's easiest to just output them all in one pass.*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NjO3wD-Sphk"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1ivTQBHWkYi_J9vWwXFd2sSWg5f2TB5T-\" width=\"400\" /></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "DWrRFI_qLmmt"
   },
   "outputs": [],
   "source": [
    "#@title **[Solution]** NFQ Agent  { form-width: \"30%\" }\n",
    "\n",
    "Transitions = collections.namedtuple('Transitions',\n",
    "                                     ['s_t', 'a_t', 'r_t', 'd_t', 's_tp1'])\n",
    "TrainingState = namedtuple('TrainingState', 'params, opt_state, step')\n",
    "\n",
    "\n",
    "class NeuralFittedQAgent(acme.Actor):\n",
    "\n",
    "  def __init__(self,\n",
    "               q_network,\n",
    "               observation_spec,\n",
    "               replay_capacity=100000,\n",
    "               epsilon=0.1,\n",
    "               batch_size=1,\n",
    "               learning_rate=3e-4):\n",
    "\n",
    "    self._observation_spec = observation_spec\n",
    "    self.epsilon = epsilon\n",
    "    self._batch_size = batch_size\n",
    "    self._replay_buffer = ReplayBuffer(replay_capacity)\n",
    "    self.last_loss = 0\n",
    "\n",
    "    # Setup Network and loss with Haiku\n",
    "    self._rng = hk.PRNGSequence(1)\n",
    "    self._q_network = hk.without_apply_rng(hk.transform(q_network))\n",
    "    \n",
    "    # Initialize network\n",
    "    dummy_observation = observation_spec.generate_value()\n",
    "    initial_params = self._q_network.init(\n",
    "        next(self._rng), dummy_observation[None, ...])\n",
    "\n",
    "    # Setup optimizer\n",
    "    self._optimizer = optax.adam(learning_rate)\n",
    "    initial_optimizer_state = self._optimizer.init(initial_params)\n",
    "\n",
    "    self._state = TrainingState(\n",
    "        params=initial_params, opt_state=initial_optimizer_state, step=0)\n",
    "\n",
    "  @functools.partial(jax.jit, static_argnums=(0,))\n",
    "  def _policy(self, params: hk.Params, rng_key: jnp.ndarray,\n",
    "              observation: jnp.ndarray, epsilon: float):\n",
    "    q_values = self._q_network.apply(params, observation[None, ...])\n",
    "    actions = rlax.epsilon_greedy(epsilon).sample(rng_key, q_values)\n",
    "    return jnp.squeeze(actions, axis=0)\n",
    "\n",
    "  def select_action(self, observation):\n",
    "    return self._policy(self._state.params, next(self._rng), observation,\n",
    "                        self.epsilon)\n",
    "\n",
    "  def q_values(self, observation):\n",
    "    return jnp.squeeze(\n",
    "        self._q_network.apply(self._state.params, observation[None, ...]),\n",
    "        axis=0)\n",
    "\n",
    "  @functools.partial(jax.jit, static_argnums=(0,))\n",
    "  def _loss(self, params: hk.Params, transitions: Transitions):\n",
    "\n",
    "    def _td_error(q_s, q_next_s, a, r, d):\n",
    "      \"\"\"TD error for a single transition.\"\"\"\n",
    "      target_s = r + d * jnp.max(q_next_s)\n",
    "      td_error = jax.lax.stop_gradient(target_s) - q_s[a]\n",
    "      # Task: think of why we are not using td_error = target_s - q_s[a]? \n",
    "      return td_error\n",
    "\n",
    "    batch_td_error = jax.vmap(_td_error)\n",
    "\n",
    "    # Compute batched Q-values [Batch, actions]\n",
    "    q_s = self._q_network.apply(params, transitions.s_t)\n",
    "    q_next_s = self._q_network.apply(params, transitions.s_tp1)\n",
    "    # Get batched td errors\n",
    "    td_errors = batch_td_error(q_s, q_next_s, transitions.a_t, transitions.r_t,\n",
    "                               transitions.d_t)\n",
    "    losses = 0.5 * td_errors**2.  # [Batch]\n",
    "\n",
    "    return jnp.mean(losses)\n",
    "\n",
    "  @functools.partial(jax.jit, static_argnums=(0,))\n",
    "  def _train_step(self, state: TrainingState, transitions: Transitions):\n",
    "    # Do one learning step on the batch of transitions\n",
    "    compute_loss_and_grad = jax.value_and_grad(self._loss)\n",
    "    loss, dloss_dparams = compute_loss_and_grad(state.params, transitions)\n",
    "    updates, new_opt_state = self._optimizer.update(dloss_dparams,\n",
    "                                                    state.opt_state)\n",
    "    new_params = optax.apply_updates(state.params, updates)\n",
    "\n",
    "    new_state = TrainingState(\n",
    "        params=new_params, opt_state=new_opt_state, step=state.step + 1)\n",
    "    return new_state, loss\n",
    "\n",
    "  def update(self):\n",
    "    if self._replay_buffer.is_ready(self._batch_size):\n",
    "      # Collect a minibatch of random transitions\n",
    "      transitions = Transitions(*self._replay_buffer.sample(self._batch_size))\n",
    "      # Compute loss and update parameters\n",
    "      self._state, self.last_loss = self._train_step(self._state, transitions)\n",
    "\n",
    "  def observe_first(self, timestep):\n",
    "    self._replay_buffer.push(timestep, None)\n",
    "\n",
    "  def observe(self, action, next_timestep):\n",
    "    self._replay_buffer.push(next_timestep, action)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "  \"\"\"A simple Python replay buffer.\"\"\"\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self._prev = None\n",
    "    self._action = None\n",
    "    self._latest = None\n",
    "    self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "  def push(self, timestep, action):\n",
    "    self._prev = self._latest\n",
    "    self._action = action\n",
    "    self._latest = timestep\n",
    "\n",
    "    if action is not None:\n",
    "      self.buffer.append(\n",
    "          (self._prev.observation, self._action, self._latest.reward,\n",
    "           self._latest.discount, self._latest.observation))\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    obs_tm1, a_tm1, r_t, discount_t, obs_t = zip(\n",
    "        *random.sample(self.buffer, batch_size))\n",
    "    return (jnp.stack(obs_tm1), jnp.asarray(a_tm1), jnp.asarray(r_t),\n",
    "            jnp.asarray(discount_t), jnp.stack(obs_t))\n",
    "\n",
    "  def is_ready(self, batch_size):\n",
    "    return batch_size <= len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQoI1y88Mfsz"
   },
   "source": [
    "### **Task: Train a NFQ agent**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "g7QmF3UGgYJa"
   },
   "outputs": [],
   "source": [
    "#@title Training the NFQ Agent.  { form-width: \"30%\" }\n",
    "epsilon = .5 #@param {type:\"number\"}\n",
    "num_episodes = 100 #@param {type:\"number\"}\n",
    "\n",
    "max_episode_length = 100\n",
    "\n",
    "# Environment\n",
    "grid = build_gridworld_task(\n",
    "    task='obstacle',\n",
    "    observation_type=ObservationType.AGENT_GOAL_POS,\n",
    "    max_episode_length=max_episode_length)\n",
    "environment, environment_spec = setup_environment(grid)\n",
    "\n",
    "# Define function approximation for the Q-values\n",
    "# i.e. Q_a(s) for a in num_actions.\n",
    "def q_network(observation: np.ndarray):\n",
    "  \"\"\"Outputs action values given an observation.\"\"\"\n",
    "  model = hk.Sequential([\n",
    "      hk.Flatten(),  # Flattens everything except the batch dimension\n",
    "      hk.nets.MLP([50, 50, environment_spec.actions.num_values])\n",
    "  ])\n",
    "  return model(observation)\n",
    "\n",
    "# Build the trainable Q-learning agent\n",
    "agent = NeuralFittedQAgent(\n",
    "    q_network,\n",
    "    environment_spec.observations,\n",
    "    epsilon=epsilon,\n",
    "    replay_capacity=100000,\n",
    "    batch_size=10,\n",
    "    learning_rate=1e-3)\n",
    "\n",
    "returns = run_loop(\n",
    "    environment=environment,\n",
    "    agent=agent,\n",
    "    num_episodes=num_episodes,\n",
    "    logger_time_delta=1.,\n",
    "    log_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJFUPzJ9qOM5"
   },
   "outputs": [],
   "source": [
    "#@title Visualise training curve { form-width: \"30%\" }\n",
    "\n",
    "# Compute rolling average over returns\n",
    "returns_avg = pd.Series(returns).rolling(10, center=True).mean()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(len(returns)), returns_avg)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total reward');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bZM2TNJ0PB6F"
   },
   "outputs": [],
   "source": [
    "#@title Evaluating the agent.  { form-width: \"30%\" }\n",
    "\n",
    "# Change epsilon to be more greedy\n",
    "agent.epsilon = 0.05\n",
    "\n",
    "# Look at a few episodes\n",
    "frames = evaluate(environment, agent, evaluation_episodes=1)\n",
    "display_video(frames, frame_repeat=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfJFgwxFqOM5",
    "tags": []
   },
   "source": [
    "## 2.2 REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwIMlbszqOM5"
   },
   "source": [
    "Learning the **policy directly**---learn a function $h$ such as:\n",
    "$$\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}}{\\sum_{b \\in \\mathcal{A}} e^{h(s, b, \\theta)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDhWSg9JqOM6"
   },
   "source": [
    "Advantages:\n",
    "- Naturally approach a determinitic policy during training\n",
    "- Learn **arbitrary** actions probabilities, instead of following a \"definition\" of optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_rmSKX9qOM6"
   },
   "source": [
    "Define performance as:\n",
    "$$J(\\theta) = v_{\\pi_\\theta}(s_0)$$\n",
    "\n",
    "Find $\\theta$ and maximize $v_{\\pi_\\theta}$, meaning the expected return value we get in $s_0$ by following $\\pi_\\theta$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d866TtxLqOM6"
   },
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/gianscarpe/eeml_summer_school_2021/master/notebooks/images/derivation.png\" width=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsUd09kUqOM6"
   },
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/gianscarpe/eeml_summer_school_2021/master/notebooks/images/reinforce.png\" width=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoZYfQKrqOM6"
   },
   "outputs": [],
   "source": [
    "from school_lib.models.reinforce import ReinforceAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FymN2mKOqOM6"
   },
   "outputs": [],
   "source": [
    "epsilon = 1. #@param {type:\"number\"}\n",
    "num_episodes = 3 #@param {type:\"number\"}\n",
    "\n",
    "max_episode_length = 200\n",
    "\n",
    "# Environment\n",
    "grid = build_gridworld_task(\n",
    "    task='simple',\n",
    "    observation_type=ObservationType.AGENT_GOAL_POS,\n",
    "    max_episode_length=max_episode_length)\n",
    "environment, environment_spec = setup_environment(grid)\n",
    "\n",
    "\n",
    "def pi_network(observation: np.ndarray):\n",
    "  \"\"\"Outputs action values given an observation.\"\"\"\n",
    "  model = hk.Sequential([\n",
    "      hk.Flatten(),  # Flattens everything except the batch dimension\n",
    "      hk.nets.MLP([50, 50, environment_spec.actions.num_values]),          \n",
    "  ])\n",
    "  return model(observation)\n",
    "\n",
    "# Build the trainable Q-learning agent\n",
    "agent = ReinforceAgent(\n",
    "    pi_network,\n",
    "    environment_spec.observations,\n",
    "    epsilon=epsilon,\n",
    "    replay_capacity=100000,\n",
    "    batch_size=10,\n",
    "    learning_rate=1e-3)\n",
    "\n",
    "returns = run_loop(\n",
    "    environment=environment,\n",
    "    agent=agent,\n",
    "    num_episodes=num_episodes,\n",
    "    logger_time_delta=1.,\n",
    "    log_loss=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do1PQhHtqOM7"
   },
   "outputs": [],
   "source": [
    "#@title Visualise the learned Q values\n",
    "\n",
    "# Evaluate the policy for every state, similar to tabular agents above.\n",
    "\n",
    "environment.reset()\n",
    "pi = np.zeros(grid._layout_dims, dtype=np.int32)\n",
    "for y in range(grid._layout_dims[0]):\n",
    "  for x in range(grid._layout_dims[1]):\n",
    "    # Hack observation to see what the Q-network would output at that point.\n",
    "    environment.set_state(x, y)\n",
    "    obs = environment.get_obs()\n",
    "    pi[y, x] = np.asarray(agent.select_action(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MV7-raRqOM7"
   },
   "outputs": [],
   "source": [
    "grid.plot_policy(pi)\n",
    "_ = plt.title(\"Policy using the agent's behaviour policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM7eCG2BqOM7"
   },
   "outputs": [],
   "source": [
    "frames = evaluate(environment, agent, evaluation_episodes=3)\n",
    "display_video(frames, frame_repeat=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Clv_QlpgoY1J",
    "tags": []
   },
   "source": [
    "# RL Lab - Part 3: Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjR8zkBdjIrB"
   },
   "source": [
    "\n",
    "<!-- <center><img src=\"https://drive.google.com/uc?id=1ivTQBHWkYi_J9vWwXFd2sSWg5f2TB5T-\" width=\"500\" /></center>  -->\n",
    "\n",
    "<center><img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnature14236/MediaObjects/41586_2015_Article_BFnature14236_Fig1_HTML.jpg\" width=\"500\" /></center> \n",
    "\n",
    "In this subsection, we will look at an advanced deep RL Agent based on the following publication, [Playing Atari with Deep Reinforcement Learning](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning), which introduced the first deep learning model to successfully learn control policies directly from high-dimensional pixel inputs using RL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BukOfOsmtSQn"
   },
   "source": [
    " ## 3.1 Create an ACME DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NbHdPc-nxO2j"
   },
   "outputs": [],
   "source": [
    "#@title Create the environment\n",
    "grid = build_gridworld_task(\n",
    "    task='simple', \n",
    "    observation_type=ObservationType.GRID,\n",
    "    max_episode_length=200)\n",
    "environment, environment_spec = setup_environment(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "3Jcjk1w6oHVX"
   },
   "outputs": [],
   "source": [
    "#@title Construct the agent and a training loop  { form-width: \"30%\" }\n",
    "\n",
    "# Build agent network module\n",
    "def _network(x):\n",
    "  model = hk.Sequential([\n",
    "      hk.Conv2D(32, kernel_shape=[4,4], stride=[2,2], padding='VALID'),\n",
    "      jax.nn.relu,\n",
    "      hk.Conv2D(64, kernel_shape=[3,3], stride=[1,1], padding='VALID'),\n",
    "      jax.nn.relu,\n",
    "      hk.Flatten(),\n",
    "      hk.nets.MLP([50, 50, environment_spec.actions.num_values])\n",
    "  ])\n",
    "  return model(x)\n",
    "\n",
    "# Create agent network pure functions and prebind the observation shape.\n",
    "hk_network: hk.Transformed = hk.without_apply_rng(hk.transform(_network))\n",
    "observation_spec = environment_spec.observations\n",
    "dummy_observation = np.zeros(observation_spec.shape, observation_spec.dtype)\n",
    "\n",
    "# Pack the network pure functions.\n",
    "# Here we also prebind a dummy observation to the network initialization\n",
    "# function. This is so that Acme can maintain compatibility with both haiku and\n",
    "# flax.\n",
    "network = acme.jax.networks.FeedForwardNetwork(\n",
    "    init=lambda rng_key: hk_network.init(rng_key, dummy_observation),\n",
    "    apply=hk_network.apply,\n",
    ")\n",
    "\n",
    "# Avoid logging from Acme\n",
    "class DummyLogger(object):\n",
    "\n",
    "  def write(self, data):\n",
    "    pass\n",
    "\n",
    "# Use library agent implementation.\n",
    "agent = dqn.DQN(\n",
    "    environment_spec=environment_spec,\n",
    "    network=network,\n",
    "    batch_size=10,\n",
    "    samples_per_insert=2,\n",
    "    epsilon=0.05,\n",
    "    min_replay_size=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acqPbd8zXH_K"
   },
   "source": [
    "## 3.2 **[Advanced]** DQN Algorithm.\n",
    "\n",
    "The following coding exercise implements the loss function described in the DQN paper. This loss function is used by a learner class to compute gradients for the parameters $\\theta_i$ of the Q-network $Q( \\cdot; \\theta_i)$:\n",
    "\n",
    "```none\n",
    "loss(params: hk.Params, target_params: hk.Params, sample: reverb.ReplaySample)\n",
    "```\n",
    "which, at iteration `i` computes the DQN loss $L_i$ on the parameters $\\theta_i$, based on a the set of target parameters $\\theta_{i-1}$ and a given batch of sampled trajectories `sample`. As described in the manuscript, the loss function is defined as:\n",
    "\n",
    "$$L_i (\\theta_i) = \\mathbb{E}_{\\color{red}{s},\\color{blue}{a} \\sim \\rho(\\cdot)} \\left[ \\left( y_i - Q(\\color{red}{s},\\color{blue}{a} ;\\theta_i) \\right)^2\\right]$$\n",
    "\n",
    "where the target $y_i$ is computed using a bootstrap value computed from Q-value network with target parameters:\n",
    "\n",
    "$$y_i = \\mathbb{E}_{\\color{red}{s'} \\sim \\mathcal{E}} \\left[ \\color{green}{r} + \\gamma \\max_{\\color{blue}{a'} \\in \\color{blue}{\\mathcal{A}}} Q(\\color{red}{s'}, \\color{blue}{a'} ; \\theta^{\\text{target}}_i) \\; | \\; \\color{red}{s}, \\color{blue}{a} \\right]$$\n",
    "\n",
    "The batch of data `sample` is prepackaged by the agent to match the sampling distributions $\\rho$ and $\\mathcal{E}$. To get the explicit data items, use the following:\n",
    "\n",
    "```none\n",
    "o_tm1, a_tm1, r_t, d_t, o_t = sample.data\n",
    "```\n",
    "\n",
    "The function is expected to return  \n",
    "* `mean_loss` is the mean of the above loss over the batched data,\n",
    "* (`keys`, `priorities`) will pair the `keys` corresponding to each batch item to the absolute TD-error used to compute the `mean_loss` above. The agent uses these to update priorities for samples in the replay buffer.\n",
    "\n",
    "\n",
    "**Note**. A full implementation of a DQN agent is outside the scope of this tutorial, but we encoruage you to explore the code (in a cell below) to understand where the learner fits with other the services used by the agent. Moreover, if you feel ambitious, we prepared a separate exercise where you are expected to implement the learner itself (see  *DQN Learner [Coding Task - Hard]*).\n",
    "\n",
    "\n",
    "\n",
    "**[Optional]**\n",
    "- use a Double-Q Learning Loss function instead of the original published loss (see [`rlax.double_q_learning`](https://github.com/deepmind/rlax/blob/870cba1ea8ad36725f4f3a790846298657b6fd4b/rlax/_src/value_learning.py#L233)) for more details.\n",
    "- for more stable optimization, use the Huber Loss instead of $L_2$, as prescribed in the manuscript (see [`rlax.huber_loss`](https://github.com/deepmind/rlax/blob/870cba1ea8ad36725f4f3a790846298657b6fd4b/rlax/_src/clipping.py#L31)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eZuigF_bD0DP"
   },
   "outputs": [],
   "source": [
    "# @title **[Solution]** DQN Learner  { form-width: \"30%\" }\n",
    "\n",
    "TrainingState =  namedtuple('TrainingState', 'params, target_params, opt_state, step')\n",
    "LearnerOutputs =  namedtuple('LearnerOutputs', 'keys, priorities')\n",
    "\n",
    "class DQNLearner(acme.Learner):\n",
    "  \"\"\"DQN learner.\"\"\"\n",
    "\n",
    "  _state: TrainingState\n",
    "\n",
    "  def __init__(self,\n",
    "               network,\n",
    "               obs_spec,\n",
    "               discount,\n",
    "               importance_sampling_exponent,\n",
    "               target_update_period,\n",
    "               data_iterator,\n",
    "               optimizer,\n",
    "               rng,\n",
    "               replay_client,\n",
    "               max_abs_reward=1.,\n",
    "               huber_loss_parameter=1.,\n",
    "               ):\n",
    "    \"\"\"Initializes the learner.\"\"\"\n",
    "\n",
    "    def loss(params: hk.Params, target_params: hk.Params,\n",
    "             sample: reverb.ReplaySample):\n",
    "      o_tm1, a_tm1, r_t, d_t, o_t, _ = sample.data  # Ignore the empty extras.\n",
    "      keys, probs = sample.info[:2]\n",
    "\n",
    "      # Forward pass.\n",
    "      q_tm1 = network.apply(params, o_tm1)\n",
    "      q_t_value = network.apply(target_params, o_t)\n",
    "      q_t_selector = network.apply(params, o_t)\n",
    "\n",
    "      # Cast and clip rewards.\n",
    "      d_t = (d_t * discount).astype(jnp.float32)\n",
    "      r_t = jnp.clip(r_t, -max_abs_reward, max_abs_reward).astype(jnp.float32)\n",
    "\n",
    "      # Compute double Q-learning n-step TD-error.\n",
    "      batch_error = jax.vmap(rlax.double_q_learning)\n",
    "      td_error = batch_error(q_tm1, a_tm1, r_t, d_t, q_t_value, q_t_selector)\n",
    "      batch_loss = rlax.huber_loss(td_error, huber_loss_parameter)\n",
    "\n",
    "      # Importance weighting.\n",
    "      importance_weights = (1. / probs).astype(jnp.float32)\n",
    "      importance_weights **= importance_sampling_exponent\n",
    "      importance_weights /= jnp.max(importance_weights)\n",
    "\n",
    "      # Reweight.\n",
    "      mean_loss = jnp.mean(importance_weights * batch_loss)  # []\n",
    "\n",
    "      priorities = jnp.abs(td_error).astype(jnp.float64)\n",
    "\n",
    "      return mean_loss, (keys, priorities)\n",
    "\n",
    "    def sgd_step(state, samples):\n",
    "      # Compute gradients on the given loss function and update the network\n",
    "      # using the optimizer provided at init time.\n",
    "      grad_fn = jax.grad(loss, has_aux=True)\n",
    "      gradients, (keys, priorities) = grad_fn(state.params, state.target_params,\n",
    "                                              samples)\n",
    "      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n",
    "      new_params = optax.apply_updates(state.params, updates)\n",
    "\n",
    "      # Update the internal state for the learner with (1) network parameters,\n",
    "      # (2) parameters of the target network, (3) the state of the optimizer,\n",
    "      # (4) Numbers of SGD steps performed by the agent.  \n",
    "      new_state = TrainingState(\n",
    "          params=new_params,\n",
    "          target_params=state.target_params,\n",
    "          opt_state=new_opt_state,\n",
    "          step=state.step + 1)\n",
    "\n",
    "      outputs = LearnerOutputs(keys=keys, priorities=priorities)\n",
    "\n",
    "      return new_state, outputs\n",
    "\n",
    "    # Internalise agent components (replay buffer, networks, optimizer).\n",
    "    self._replay_client = replay_client\n",
    "    self._iterator = data_iterator\n",
    "\n",
    "    # Since sampling is base on a priority experience replay, we need to pass\n",
    "    # the absolute td-loss values to the replay client to update priorities\n",
    "    # accordingly.\n",
    "    def update_priorities(outputs: LearnerOutputs):\n",
    "      for key, priority in zip(outputs.keys, outputs.priorities):\n",
    "        self._replay_client.mutate_priorities(\n",
    "            table='priority_table', \n",
    "            updates={key: priority})\n",
    "        \n",
    "    self._update_priorities = update_priorities\n",
    "\n",
    "    # Internalise the hyperparameters.\n",
    "    self._target_update_period = target_update_period\n",
    "\n",
    "    # Internalise logging/counting objects.\n",
    "    self._counter = counting.Counter()\n",
    "    self._logger = loggers.TerminalLogger('learner', time_delta=1.)\n",
    "\n",
    "    # Initialise parameters and optimiser state.\n",
    "    def initialization_fn(values):\n",
    "      values = tree_util.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), values)\n",
    "      # Add batch dim.\n",
    "      return tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), values)\n",
    "\n",
    "    initial_params = network.init(next(rng))\n",
    "    initial_target_params = initial_params\n",
    "    initial_opt_state = optimizer.init(initial_params)\n",
    "\n",
    "    self._state = TrainingState(\n",
    "        params=initial_params,\n",
    "        target_params=initial_target_params,\n",
    "        opt_state=initial_opt_state,\n",
    "        step=0)\n",
    "\n",
    "    self._forward = jax.jit(network.apply)\n",
    "    self._sgd_step = jax.jit(sgd_step)\n",
    "    \n",
    "  def step(self):\n",
    "    samples = next(self._iterator)\n",
    "    # Do a batch of SGD.\n",
    "    self._state, outputs = self._sgd_step(self._state, samples)\n",
    "\n",
    "    # Update our counts and record it.\n",
    "    result = self._counter.increment(steps=1)\n",
    "\n",
    "    # Periodically update target network parameters.\n",
    "    if self._state.step % self._target_update_period == 0:\n",
    "      self._state = self._state._replace(target_params=self._state.params)\n",
    "\n",
    "    # Update priorities in replay.\n",
    "    self._update_priorities(outputs)\n",
    "\n",
    "    # Write to logs.\n",
    "    self._logger.write(result)\n",
    "\n",
    "  def get_variables(self):\n",
    "    \"\"\"Network variables after a number of SGD steps.\"\"\"\n",
    "    return self._state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ywObWtqgaSXx"
   },
   "outputs": [],
   "source": [
    "# @title DQN Agent implementation (use for reference only) { form-width: \"30%\" }\n",
    "class DQN(acme.Actor):\n",
    "  def __init__(\n",
    "    self,\n",
    "    environment_spec,\n",
    "    network,\n",
    "    batch_size=256,\n",
    "    prefetch_size=4,\n",
    "    target_update_period=100,\n",
    "    samples_per_insert=32.0,\n",
    "    min_replay_size=1000,\n",
    "    max_replay_size=1000000,\n",
    "    importance_sampling_exponent=0.2,\n",
    "    priority_exponent=0.6,\n",
    "    n_step=5,\n",
    "    epsilon=0.,\n",
    "    learning_rate=1e-3,\n",
    "    discount=0.99,\n",
    "  ):\n",
    "    # Create a replay server to add data to. This is initialized as a\n",
    "    # table, and a Learner (defined separately) will be in charge of updating\n",
    "    # sample priorities based on the corresponding learner loss. \n",
    "    replay_table = reverb.Table(\n",
    "        name='priority_table',\n",
    "        sampler=reverb.selectors.Prioritized(priority_exponent),\n",
    "        remover=reverb.selectors.Fifo(),\n",
    "        max_size=max_replay_size,\n",
    "        rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "        signature=adders.NStepTransitionAdder.signature(environment_spec))\n",
    "    self._server = reverb.Server([replay_table], port=None)\n",
    "    address = f'localhost:{self._server.port}'\n",
    "\n",
    "    # Use ACME reverb adder as a tool to add transition data into the replay\n",
    "    # buffer defined above.\n",
    "    self._adder = adders.NStepTransitionAdder(\n",
    "        client=reverb.Client(address),\n",
    "        n_step=n_step,\n",
    "        discount=discount)\n",
    "\n",
    "    # ACME datasets provides an interface to easily sample from a replay server.\n",
    "    dataset = datasets.make_reverb_dataset(\n",
    "        server_address=address,\n",
    "        batch_size=batch_size,\n",
    "        prefetch_size=prefetch_size)\n",
    "    data_iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    # Create a learner that updates the parameters (and initializes them).\n",
    "    self._learner = DQNLearner(\n",
    "        network=network,\n",
    "        obs_spec=environment_spec.observations,\n",
    "        rng=hk.PRNGSequence(1),\n",
    "        optimizer=optax.adam(learning_rate),\n",
    "        discount=discount,\n",
    "        importance_sampling_exponent=importance_sampling_exponent,\n",
    "        target_update_period=target_update_period,\n",
    "        data_iterator=data_iterator,\n",
    "        replay_client=reverb.Client(address),\n",
    "    )\n",
    "    \n",
    "    # Create a feed forward actor that obtains its variables from the DQNLearner\n",
    "    # above.\n",
    "    def policy(params, key, observation):\n",
    "      action_values = network.apply(params, observation)\n",
    "      return rlax.epsilon_greedy(epsilon).sample(key, action_values)\n",
    "\n",
    "    self._policy = policy\n",
    "    self._rng = hk.PRNGSequence(1)\n",
    " \n",
    "    # We'll ignore the first min_observations when determining whether to take\n",
    "    # a step and we'll do so by making sure num_observations >= 0.\n",
    "    self._num_observations = -max(batch_size, min_replay_size)\n",
    "\n",
    "    observations_per_step = float(batch_size) / samples_per_insert\n",
    "    if observations_per_step >= 1.0:\n",
    "      self._observations_per_update = int(observations_per_step)\n",
    "      self._learning_steps_per_update = 1\n",
    "    else:\n",
    "      self._observations_per_update = 1\n",
    "      self._learning_steps_per_update = int(1.0 / observations_per_step)\n",
    "\n",
    "  def select_action(self, observation):\n",
    "    observation = tree_util.tree_map(lambda x: jnp.expand_dims(x, axis=0), \n",
    "                                     observation)\n",
    "    \n",
    "    key = next(self._rng)\n",
    "    params = self._learner.get_variables()\n",
    "    action = self._policy(params, key, observation)\n",
    "    action = tree_util.tree_map(lambda x: np.array(x).squeeze(axis=0), action)\n",
    "    return action \n",
    "\n",
    "  def observe_first(self, timestep):\n",
    "    self._adder.add_first(timestep)\n",
    "\n",
    "  def observe(self, action, next_timestep):\n",
    "    self._num_observations += 1\n",
    "    self._adder.add(action, next_timestep)\n",
    "\n",
    "  def update(self):\n",
    "    # Only allow updates after some minimum number of observations have been and\n",
    "    # then at some period given by observations_per_update.\n",
    "    if (self._num_observations >= 0 and\n",
    "        self._num_observations % self._observations_per_update == 0):\n",
    "      self._num_observations = 0\n",
    "\n",
    "      # Run a number of learner steps (usually gradient steps).\n",
    "      for _ in range(self._learning_steps_per_update):\n",
    "        self._learner.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0iBoBqLvcy14"
   },
   "outputs": [],
   "source": [
    "# @title Run a training loop  { form-width: \"30%\" }\n",
    "\n",
    "# Run a `num_episodes` training episodes.\n",
    "# Rerun this cell until the agent has learned the given task.\n",
    "\n",
    "grid = build_gridworld_task(\n",
    "    task='simple', \n",
    "    observation_type=ObservationType.GRID, \n",
    "    max_episode_length=100,\n",
    ")\n",
    "environment, environment_spec = setup_environment(grid)\n",
    "\n",
    "# Build agent networks\n",
    "def network(x):\n",
    "  model = hk.Sequential([\n",
    "      hk.Conv2D(32, kernel_shape=[4,4], stride=[2,2], padding='VALID'),\n",
    "      jax.nn.relu,\n",
    "      hk.Conv2D(64, kernel_shape=[3,3], stride=[1,1], padding='VALID'),\n",
    "      jax.nn.relu,\n",
    "      hk.Flatten(),\n",
    "      hk.nets.MLP([50, 50, environment_spec.actions.num_values])\n",
    "  ])\n",
    "  return model(x)\n",
    "\n",
    "\n",
    "# Create agent network pure functions and prebind the observation shape.\n",
    "hk_network: hk.Transformed = hk.without_apply_rng(hk.transform(network))\n",
    "observation_spec = environment_spec.observations\n",
    "dummy_observation = np.zeros(observation_spec.shape, observation_spec.dtype)\n",
    "# Create agent network pure functions and prebind the observation shape.\n",
    "\n",
    "network = acme.jax.networks.FeedForwardNetwork(\n",
    "    init=lambda rng_key: hk_network.init(rng_key, dummy_observation),\n",
    "    apply=hk_network.apply,\n",
    ")\n",
    "\n",
    "\n",
    "agent = DQN(\n",
    "    environment_spec=environment_spec,\n",
    "    network=network,\n",
    "    batch_size=16,\n",
    "    samples_per_insert=2,\n",
    "    epsilon=0.1,\n",
    "    min_replay_size=100)\n",
    "\n",
    "returns = run_loop(environment=environment, agent=agent, num_episodes=200, \n",
    "    logger_time_delta=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPcCF1tyqOM_"
   },
   "outputs": [],
   "source": [
    "#@title Visualise training curve { form-width: \"30%\" }\n",
    "\n",
    "# Compute rolling average over returns\n",
    "returns_avg = pd.Series(returns).rolling(10, center=True).mean()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(len(returns)), returns_avg)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total reward');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJi7LDrn0eO4"
   },
   "source": [
    "### OpenAI-Gym classical controls\n",
    "\n",
    "Here we show that you can apply what you learned to other environments such as Cartpole in [Gym](https://gym.openai.com/).\n",
    "\n",
    "\n",
    "<center><img src=\"https://user-images.githubusercontent.com/10624937/42135683-dde5c6f0-7d13-11e8-90b1-8770df3e40cf.gif\" height=\"250\" /></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrTfjf5SqOM_"
   },
   "outputs": [],
   "source": [
    "from school_lib.tools import evaluate_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97sWuPFAqOM_",
    "tags": []
   },
   "source": [
    "## Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "DIERzZVk0xIh",
    "outputId": "9a692c86-a7f6-46d6-d67b-8483cf88f57f"
   },
   "outputs": [],
   "source": [
    "#@title Construct the agent and run the training loop { form-width: \"30%\" }\n",
    "\n",
    "mlp_1 = 50 #@param {type:\"number\"}\n",
    "mlp_2 = 50 #@param {type:\"number\"}\n",
    "\n",
    "env = gym_wrapper.GymWrapper(gym.make('CartPole-v0'))\n",
    "env = wrappers.SinglePrecisionWrapper(env)\n",
    "\n",
    "environment, environment_spec = setup_environment(env)\n",
    "\n",
    "# Build agent networks\n",
    "def network(x):\n",
    "  model = hk.Sequential([\n",
    "      hk.Flatten(),\n",
    "      hk.nets.MLP([mlp_1, mlp_2, environment_spec.actions.num_values])\n",
    "  ])\n",
    "  return model(x)\n",
    "\n",
    "# Create agent network pure functions and prebind the observation shape.\n",
    "hk_network: hk.Transformed = hk.without_apply_rng(hk.transform(network))\n",
    "observation_spec = environment_spec.observations\n",
    "dummy_observation = np.zeros(observation_spec.shape, observation_spec.dtype)\n",
    "# Create agent network pure functions and prebind the observation shape.\n",
    "\n",
    "network = acme.jax.networks.FeedForwardNetwork(\n",
    "    init=lambda rng_key: hk_network.init(rng_key, dummy_observation),\n",
    "    apply=hk_network.apply,\n",
    ")\n",
    "\n",
    "\n",
    "agent = DQN(\n",
    "    environment_spec=environment_spec,\n",
    "    network=network,\n",
    "    batch_size=16,\n",
    "    samples_per_insert=2,\n",
    "    epsilon=0.1,\n",
    "    min_replay_size=100)\n",
    "\n",
    "returns = run_loop(environment=environment, agent=agent, num_episodes=300, \n",
    "         logger_time_delta=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dDmLcICc98Z8"
   },
   "outputs": [],
   "source": [
    "#@title Visualise training curve { form-width: \"30%\" }\n",
    "\n",
    "# Compute rolling average over returns\n",
    "returns_avg = pd.Series(returns).rolling(10, center=True).mean()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(len(returns)), returns_avg)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total reward');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIuAVOkDqONA"
   },
   "outputs": [],
   "source": [
    "frames = evaluate_gym(environment, agent, evaluation_episodes=1)\n",
    "display_video(frames, frame_repeat=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00-WdSOTqONA",
    "tags": []
   },
   "source": [
    "### Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrNqOqVuqONA"
   },
   "outputs": [],
   "source": [
    "epsilon = 1. #@param {type:\"number\"}\n",
    "num_episodes = 3 #@param {type:\"number\"}\n",
    "\n",
    "max_episode_length = 200 #@param {type:\"number\"}\n",
    "env = gym_wrapper.GymWrapper(gym.make('CartPole-v0'))\n",
    "\n",
    "environment, environment_spec = setup_environment(env)\n",
    "\n",
    "\n",
    "def pi_network(observation: np.ndarray):\n",
    "  \"\"\"Outputs action values given an observation.\"\"\"\n",
    "  model = hk.Sequential([\n",
    "      hk.Flatten(),  # Flattens everything except the batch dimension\n",
    "      hk.nets.MLP([100, 100, environment_spec.actions.num_values]),          \n",
    "  ])\n",
    "  return model(observation)\n",
    "\n",
    "\n",
    "# Build the trainable Q-learning agent\n",
    "agent = ReinforceAgent(\n",
    "    pi_network,\n",
    "    environment_spec.observations,\n",
    "    epsilon=epsilon,\n",
    "    replay_capacity=100000,\n",
    "    batch_size=10,\n",
    "    learning_rate=1e-3)\n",
    "\n",
    "returns = run_loop(environment=environment, agent=agent, num_episodes=300, \n",
    "         logger_time_delta=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7XZViONqONB"
   },
   "outputs": [],
   "source": [
    "#@title Visualise training curve { form-width: \"30%\" }\n",
    "\n",
    "\n",
    "# Compute rolling average over returns\n",
    "returns_avg = pd.Series(returns).rolling(10, center=True).mean()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(len(returns)), returns_avg)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total reward');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ps1_Ze5SqONB"
   },
   "outputs": [],
   "source": [
    "frames = evaluate_gym(environment, agent, evaluation_episodes=1)\n",
    "display_video(frames, frame_repeat=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj43lzJgqONB",
    "tags": []
   },
   "source": [
    "## Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qWzYH9MqONB"
   },
   "outputs": [],
   "source": [
    "!wget www.atarimania.com/roms/Roms.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayXbyW8yqONB"
   },
   "outputs": [],
   "source": [
    "!apt install unrar\n",
    "!unrar e Roms.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLm0cOu3qONB"
   },
   "outputs": [],
   "source": [
    "!pip install atari-py\n",
    "!python -m atari_py.import_roms ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aL63d-I7qONB"
   },
   "outputs": [],
   "source": [
    "from acme.jax import utils\n",
    "from acme.jax import networks as networks_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cM7u-VaqONC"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_environment(evaluation: bool = False,\n",
    "                     level: str = 'PongNoFrameskip-v4'):\n",
    "  env = gym.make(level, full_action_space=True)\n",
    "\n",
    "  max_episode_len = 108_000 if evaluation else 50_000\n",
    "\n",
    "  return wrappers.wrap_all(env, [\n",
    "      wrappers.GymAtariAdapter,\n",
    "      functools.partial(\n",
    "          wrappers.AtariWrapper,\n",
    "          to_float=True,\n",
    "          max_episode_len=max_episode_len,\n",
    "          zero_discount_on_life_loss=True,\n",
    "      ),\n",
    "      wrappers.SinglePrecisionWrapper,\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYB0wo2BqONC"
   },
   "outputs": [],
   "source": [
    "level = \"MsPacman-v0\" #@param {type:\"string\"}\n",
    "env = make_environment(level=level) \n",
    "spec = acme.make_environment_spec(env)\n",
    "\n",
    "def network(x):\n",
    "    model = hk.Sequential([\n",
    "      hk.Conv2D(32, kernel_shape=[4,4], stride=[2,2], padding='VALID'),\n",
    "      jax.nn.relu,\n",
    "      hk.Conv2D(64, kernel_shape=[3,3], stride=[1,1], padding='VALID'),\n",
    "      jax.nn.relu,\n",
    "      hk.Flatten(),\n",
    "      hk.nets.MLP([50, 50, spec.actions.num_values])\n",
    "  ])\n",
    "    return model(x)\n",
    "\n",
    "# Make network purely functional\n",
    "network_hk = hk.without_apply_rng(hk.transform(network, apply_rng=True))\n",
    "dummy_obs = utils.add_batch_dim(utils.zeros_like(spec.observations))\n",
    "\n",
    "network = networks_lib.FeedForwardNetwork(\n",
    "    init=lambda rng: network_hk.init(rng, dummy_obs),\n",
    "    apply=network_hk.apply)\n",
    "\n",
    "# Construct the agent.\n",
    "agent = dqn.DQN(\n",
    "    environment_spec=spec,\n",
    "    network=network,\n",
    "    batch_size=10,\n",
    "    samples_per_insert=2,\n",
    "    min_replay_size=10)\n",
    "\n",
    "returns = run_loop(environment=env, agent=agent, num_episodes=10, \n",
    "         logger_time_delta=0.2)\n",
    "# Compute rolling average over returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kR4okBRfqONC"
   },
   "outputs": [],
   "source": [
    "frames = evaluate_gym(env, agent, evaluation_episodes=1)\n",
    "display_video(frames, frame_repeat=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzqrYxAtH11S"
   },
   "source": [
    "# Want to learn more?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odBz1OO0JIXY"
   },
   "source": [
    "\n",
    "This Colab is based on the [EEML 2020 RL practical](https://github.com/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb) by Feryal Behbahani & Gheorghe Comanici. \n",
    "\n",
    "\n",
    "Books and lecture notes\n",
    "*   [Reinforcement Learning: an Introduction by Sutton & Barto](http://incompleteideas.net/book/RLbook2018.pdf)\n",
    "* [Algorithms for Reinforcement Learning by Csaba Szepesvari](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)\n",
    "\n",
    "\n",
    "Lectures and course \n",
    "*   [RL Course by David Silver](https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-)\n",
    "*   [Reinforcement Learning Course | UCL & DeepMind](https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb)\n",
    "*   [Emma Brunskill Stanford RL Course](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u)\n",
    "*   [RL Course on Coursera by Martha White & Adam White](https://www.coursera.org/specializations/reinforcement-learning)\n",
    "\n",
    "More practical:\n",
    "* [Spinning Up in Deep RL by Josh Achiam](https://spinningup.openai.com/en/latest/)\n",
    "*   [Acme white paper](http://go/arxiv/2006.00979)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copia di EEML2021_RL_Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
